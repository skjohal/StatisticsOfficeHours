[["index.html", "Statistics ‘Office Hours’: Introduction to Common Statistical Methods in R 1 Introduction", " Statistics ‘Office Hours’: Introduction to Common Statistical Methods in R Simran K. Johal and Danielle Siegel 1 Introduction Thank you for registering for our “office hours” for statistical analysis! We are excited to help you with your analysis for your research project, and hope that we can provide you with useful information. The focus of the in-person office hours is to answer your specific questions about your statistical analyses, and so we will not be providing lectures about how to conduct different statistical tests in R. Therefore, we have provided you with this resource, which shows you how to conduct common statistical analyses, and interpret and visualize the results. If you have any questions, feel free to contact Simran (skjohal@ucdavis.edu) or Danielle (dpsiegel@ucdavis.edu) "],["dataintro.html", "2 Introduction to the Data", " 2 Introduction to the Data The dataset that we will be using throughout our examples is a dataset on the Big 5 Personality Test, taken from the Open-Source Psychometrics Project. During the Big 5 Personality Test, participants answer questions from the Big-Five Factor Markers from the International Personality Item Pool (IPIP). This contains fifty items that measure the Big 5 personality traits (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism), and participants rate how true each item is to them on a five-point scale (1 = Disagree, 5 = Agree). To date, over 19,000 people have completed this personality assessment. To make this dataset more manageable for our purposes, I took a stratified random sample of 500 participants, with 100 people from each continent (Africa, Americas, Asia, Europe, and Oceania). Let’s load in the dataset, and take a quick look at what it contains. Make sure to change the file path in read.csv to wherever you are keeping the dataset. big5 = read.csv(&quot;Big5_Workshop.csv&quot;) To get an idea of what this data looks like, we can use head to view the first few rows, or summary to get an idea of the type for each column. head(big5) summary(big5) ## id race age engnat ## Min. : 1.0 Min. : 1.000 Min. :13.00 Min. :1.000 ## 1st Qu.:125.8 1st Qu.: 3.000 1st Qu.:19.00 1st Qu.:1.000 ## Median :250.5 Median : 3.000 Median :23.00 Median :1.000 ## Mean :250.5 Mean : 6.168 Mean :26.73 Mean :1.465 ## 3rd Qu.:375.2 3rd Qu.:11.000 3rd Qu.:32.00 3rd Qu.:2.000 ## Max. :500.0 Max. :13.000 Max. :71.00 Max. :2.000 ## NA&#39;s :7 NA&#39;s :3 ## gender hand source country ## Min. :1.000 Min. :1.000 Min. :1.000 Length:500 ## 1st Qu.:1.000 1st Qu.:1.000 1st Qu.:1.000 Class :character ## Median :2.000 Median :1.000 Median :1.000 Mode :character ## Mean :1.609 Mean :1.137 Mean :1.746 ## 3rd Qu.:2.000 3rd Qu.:1.000 3rd Qu.:2.000 ## Max. :3.000 Max. :3.000 Max. :5.000 ## NA&#39;s :1 NA&#39;s :4 ## E1 E2 E3 E4 E5 ## Min. :1.00 Min. :1.000 Min. :1.00 Min. :1.000 Min. :1.000 ## 1st Qu.:1.00 1st Qu.:2.000 1st Qu.:3.00 1st Qu.:2.000 1st Qu.:2.750 ## Median :3.00 Median :3.000 Median :4.00 Median :3.000 Median :4.000 ## Mean :2.58 Mean :2.784 Mean :3.43 Mean :3.098 Mean :3.436 ## 3rd Qu.:3.00 3rd Qu.:4.000 3rd Qu.:4.00 3rd Qu.:4.000 3rd Qu.:4.250 ## Max. :5.00 Max. :5.000 Max. :5.00 Max. :5.000 Max. :5.000 ## ## E6 E7 E8 E9 ## Min. :1.000 Min. :1.000 Min. :1.000 Min. :1.000 ## 1st Qu.:2.000 1st Qu.:2.000 1st Qu.:2.000 1st Qu.:2.000 ## Median :2.000 Median :3.000 Median :3.000 Median :3.000 ## Mean :2.622 Mean :2.862 Mean :3.332 Mean :3.148 ## 3rd Qu.:4.000 3rd Qu.:4.000 3rd Qu.:4.250 3rd Qu.:4.000 ## Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 ## ## E10 N1 N2 N3 ## Min. :1.000 Min. :1.000 Min. :1.000 Min. :1.000 ## 1st Qu.:3.000 1st Qu.:2.000 1st Qu.:2.000 1st Qu.:3.000 ## Median :4.000 Median :3.000 Median :3.000 Median :4.000 ## Mean :3.622 Mean :3.172 Mean :3.258 Mean :3.822 ## 3rd Qu.:5.000 3rd Qu.:4.000 3rd Qu.:4.000 3rd Qu.:5.000 ## Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 ## ## N4 N5 N6 N7 ## Min. :1.000 Min. :1.000 Min. :1.000 Min. :1.000 ## 1st Qu.:2.000 1st Qu.:2.000 1st Qu.:2.000 1st Qu.:2.000 ## Median :3.000 Median :3.000 Median :3.000 Median :3.000 ## Mean :2.834 Mean :3.004 Mean :2.978 Mean :3.182 ## 3rd Qu.:4.000 3rd Qu.:4.000 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 ## ## N8 N9 N10 A1 A2 ## Min. :1.000 Min. :1.000 Min. :1.000 Min. :1.00 Min. :1.000 ## 1st Qu.:2.000 1st Qu.:2.000 1st Qu.:2.000 1st Qu.:1.00 1st Qu.:3.000 ## Median :3.000 Median :3.000 Median :3.000 Median :2.00 Median :4.000 ## Mean :2.824 Mean :3.214 Mean :2.886 Mean :2.38 Mean :3.916 ## 3rd Qu.:4.000 3rd Qu.:4.000 3rd Qu.:4.000 3rd Qu.:4.00 3rd Qu.:5.000 ## Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.00 Max. :5.000 ## ## A3 A4 A5 A6 A7 ## Min. :1.00 Min. :1.00 Min. :1.0 Min. :1.00 Min. :1.000 ## 1st Qu.:1.00 1st Qu.:4.00 1st Qu.:1.0 1st Qu.:3.00 1st Qu.:1.000 ## Median :2.00 Median :4.00 Median :2.0 Median :4.00 Median :2.000 ## Mean :2.15 Mean :3.97 Mean :2.2 Mean :3.86 Mean :2.178 ## 3rd Qu.:3.00 3rd Qu.:5.00 3rd Qu.:3.0 3rd Qu.:5.00 3rd Qu.:3.000 ## Max. :5.00 Max. :5.00 Max. :5.0 Max. :5.00 Max. :5.000 ## ## A8 A9 A10 C1 C2 ## Min. :1.00 Min. :1.000 Min. :1.00 Min. :1.00 Min. :1.000 ## 1st Qu.:3.00 1st Qu.:3.000 1st Qu.:3.00 1st Qu.:2.00 1st Qu.:2.000 ## Median :4.00 Median :4.000 Median :4.00 Median :3.00 Median :3.000 ## Mean :3.73 Mean :3.944 Mean :3.74 Mean :3.23 Mean :2.982 ## 3rd Qu.:4.00 3rd Qu.:5.000 3rd Qu.:5.00 3rd Qu.:4.00 3rd Qu.:4.000 ## Max. :5.00 Max. :5.000 Max. :5.00 Max. :5.00 Max. :5.000 ## ## C3 C4 C5 C6 ## Min. :1.000 Min. :1.000 Min. :1.000 Min. :1.000 ## 1st Qu.:3.000 1st Qu.:2.000 1st Qu.:2.000 1st Qu.:2.000 ## Median :4.000 Median :2.000 Median :3.000 Median :3.000 ## Mean :3.932 Mean :2.642 Mean :2.804 Mean :2.946 ## 3rd Qu.:5.000 3rd Qu.:4.000 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 ## ## C7 C8 C9 C10 O1 ## Min. :1.000 Min. :1.000 Min. :1.000 Min. :1.000 Min. :1.00 ## 1st Qu.:3.000 1st Qu.:1.000 1st Qu.:2.000 1st Qu.:3.000 1st Qu.:3.00 ## Median :4.000 Median :3.000 Median :3.000 Median :4.000 Median :4.00 ## Mean :3.626 Mean :2.542 Mean :3.168 Mean :3.654 Mean :3.61 ## 3rd Qu.:5.000 3rd Qu.:3.000 3rd Qu.:4.000 3rd Qu.:4.000 3rd Qu.:5.00 ## Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.00 ## ## O2 O3 O4 O5 O6 ## Min. :1.00 Min. :1.000 Min. :1.0 Min. :1.000 Min. :1.000 ## 1st Qu.:1.00 1st Qu.:4.000 1st Qu.:1.0 1st Qu.:3.000 1st Qu.:1.000 ## Median :2.00 Median :4.000 Median :2.0 Median :4.000 Median :1.000 ## Mean :2.14 Mean :4.118 Mean :2.1 Mean :3.902 Mean :1.786 ## 3rd Qu.:3.00 3rd Qu.:5.000 3rd Qu.:3.0 3rd Qu.:5.000 3rd Qu.:2.000 ## Max. :5.00 Max. :5.000 Max. :5.0 Max. :5.000 Max. :5.000 ## ## O7 O8 O9 O10 ## Min. :1.000 Min. :1.000 Min. :1.000 Min. :1.000 ## 1st Qu.:4.000 1st Qu.:2.000 1st Qu.:4.000 1st Qu.:3.000 ## Median :4.000 Median :3.000 Median :4.000 Median :4.000 ## Mean :4.164 Mean :3.092 Mean :4.132 Mean :4.044 ## 3rd Qu.:5.000 3rd Qu.:4.000 3rd Qu.:5.000 3rd Qu.:5.000 ## Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 ## ## AgreeablenessSum OpennessSum ExtraversionSum ConscSum ## Min. :21.00 Min. :19.00 Min. :22.00 Min. :20.00 ## 1st Qu.:30.00 1st Qu.:31.00 1st Qu.:29.00 1st Qu.:29.00 ## Median :32.00 Median :33.00 Median :31.00 Median :31.00 ## Mean :32.07 Mean :33.09 Mean :30.91 Mean :31.53 ## 3rd Qu.:34.00 3rd Qu.:36.00 3rd Qu.:33.00 3rd Qu.:34.00 ## Max. :48.00 Max. :46.00 Max. :46.00 Max. :44.00 ## ## NeuroSum continent ## Min. :10.00 Length:500 ## 1st Qu.:26.00 Class :character ## Median :31.00 Mode :character ## Mean :31.17 ## 3rd Qu.:36.00 ## Max. :48.00 ## This dataset contains 65 columns. The first few columns contain demographic information: id: The participant id. Tells different people apart from each other race: Participant’s self-reported racial identity age: Participant’s self-reported age engnat: Whether or not English is the participant’s native language (1 = Yes, 2 = No) gender: Participant’s self-reported gender (1 = Male, 2 = Female, 3 = Other) hand: Participant’s handedness (1 = Right, 2 = Left, 3 = Both) The next few columns contain participant’s responses to the individual items on the IPIP (e.g., E1 is the participant’s response to the first Extraversion question). I have combined these columns into summary columns by adding up their responses within each Big 5 Factor, such that higher scores in these columns represent higher levels of that factor. AgreeablenessSum: Sum of Agreeableness items OpennessSum: Sum of Openness items ExtraversionSum: Sum of Extraversion items ConscSum: Sum of Conscietiousness items NeuroSum:Sum of Neuroticism items We use sums in our examples instead of the individual items because the individual items were answered on a 1-5 scale, so they were answered on an ordinal data scale. However, almost all of the analyses we cover in this resource assume that the outcome variable is continuous, so taking the sum allows us to approximate a more continuous variable! "],["descriptives.html", "3 Descriptive Statistics 3.1 Descriptive Statistics", " 3 Descriptive Statistics 3.1 Descriptive Statistics Before running your final statistical analysis, it’s useful to look at descriptive statistics and visualizations of your variables. This can help you avoid running statistics that may not be appropriate for your data, such as running a linear correlation on two variables that have a nonlinear relationship. 3.1.1 Descriptives for Continuous Data For this example, we’ll be using the variable AgreeablenessSum from our example Big 5 Data Set. It is a simple sum score of the ten scale items (\\(A1\\)-\\(A10\\)) that measure Agreeableness. To get started, we can use the summary function on AgreeablenessSum to get some basic descriptives. Recall that we can specify a specific column within a dataset with the format: dataset_name\\$variable_name summary(big5$AgreeablenessSum) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 21.00 30.00 32.00 32.07 34.00 48.00 The output of this function returns the minimum value, information about the median and interquartile range, the mean, and the maximum value. If there were missing values, they would also appear here as an “NA” column. These data are squeaky clean for demonstrative purposes, but looking at the summary output can also be useful for data cleaning. For example: Do the minimums and maximums make sense for the scale used? How much missing data is there? Does that seem reasonable for how your data were collected? There may be other descriptive stats you’re interested in, such as measures of variability (variance, standard deviation). The code block below provides several functions that return different descriptive statistics: mean(big5$AgreeablenessSum) ##mean ## [1] 32.068 var(big5$AgreeablenessSum) ##variance ## [1] 13.314 sd(big5$AgreeablenessSum) ##standard deviation ## [1] 3.648836 IQR(big5$AgreeablenessSum) ##interquartile range ## [1] 4 sum(is.na(big5$AgreeablenessSum)) ##total number of missing observations ## [1] 0 table(big5$AgreeablenessSum) ##frequency tables ## ## 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 48 ## 2 1 3 5 2 15 19 27 38 46 55 79 51 46 36 22 16 14 9 5 2 5 1 1 3.1.2 Descriptive Plots for Continuous Variables While looking at numeric descriptions of variables can be useful, it’s much easier to plot variables to look at their distributions. For example, we can create a boxplot to visualize nearly all of the information provided in the summary output: boxplot(big5$AgreeablenessSum, horizontal = TRUE, ##orient the plot horizontally main = &quot;Boxplot of Agreeableness Variable&quot;, ##name the plot xlab = &quot;Agreeableness Sum Score&quot;) ##label the x-axis We could also look at the shape of our distribution using a histogram, or density plot: ##create a histogram hist(big5$AgreeablenessSum) ##Use the breaks argument to adjust bins by providing: ##A vector of user-specified cutoff values: hist(big5$AgreeablenessSum, breaks=c(15,22,23,36,42,60)) ##A function, like seq(), that computes the breakpoints: hist(big5$AgreeablenessSum, breaks=seq(21, 48, by = 3)) ##A value representing the number of desired bins: hist(big5$AgreeablenessSum, breaks=22) ##create a density plot density_values &lt;- density(big5$AgreeablenessSum) plot(density_values) While these plots get the job done to look at our distributions, they may not be as presentable as we’d like for posters of presentations. We can use a special set of functions in R’s ggplot2 package to make more attractive graphs! In ggplot, we build graphs in layers. Let’s say I want to remake my histogram of Agreeableness in ggplot. First, we need to create the axes and background of the plot: library(ggplot2) ggplot(data= big5, aes(x=AgreeablenessSum)) In this first line of code, we’ve specified the dataset we’d like to work with, as well as the variables we’d like to plot in the aes() function. The argument x used in the aes() function corresponds to the x-axis of our graph. Now, we’re going to create the second layer of a graph using a geom. Geoms are specific to ggplot, and help us define the graph we’d like to create: ggplot(data= big5, aes(x=AgreeablenessSum))+ ##our original line of code # add a plus sign at the end of each line before adding a new line geom_histogram(bins=25)## new line Now we’ve created a basic histogram in ggplot! We can continue to layer code onto our plots to get them to look the way we’d like: ggplot(data= big5, aes(x=AgreeablenessSum))+ geom_histogram(bins=25, alpha = .6, color=&quot;black&quot;, size = .1)+ ggtitle(&quot;My Histogram&quot;)+ labs(y = &quot;Frequency&quot;, x=&quot;Agreeableness&quot;)+ theme_classic()+ theme(plot.title = element_text(face = &quot;bold&quot;, hjust = .5)) The ggplot2 package is very comprehensive, and cannot be gone over in its entirety here. However, I encourage you to check out the link below and try to google any specific issues you might be having. GGplot is fairly well documented, and there are a lot of good resources out there! There will also be relevant plots and code throughout this handout. Click here for ggplot2 resources 3.1.3 Categorical Variables We can also look at descriptive statistics for categorical variables, although arithmetic values like means will no longer be available to us. Instead, we can visualize frequency of levels within a categorical variable. For instance, maybe we are interested in looking at how many observations in our data come from each continent. We can do this using the table() function: table(big5$continent) ## ## Africa Americas Asia Europe Oceania ## 100 100 100 100 100 We are also often interested in looking at the breakdown of our sample across multiple categorical variables. How many folks within each continent are male or female? If we include two categorical variables in our table function, we will get table that returns our sample broken down by both groups. In the following output, our sample from Africa has 35 men (category 1) and 65 women (category 2). table(big5$continent, big5$gender) ## ## 1 2 3 ## Africa 35 65 0 ## Americas 38 59 2 ## Asia 49 50 1 ## Europe 38 62 0 ## Oceania 38 62 0 3.1.4 Plots of Categorical Data Barplots are a usual way to visualize and compare categorical groups. Here is a basic template for a frequency bar in ggplot: ggplot(data=subset(big5, !is.na(engnat)), aes(as.factor(engnat)))+ geom_bar()+ ##geom that specifies I want a barplot ggtitle(&quot;Native English Speaker Status&quot;)+ ##title labs(x= &quot;Native English Speaker&quot;)+ ##axis labels theme_classic()+ ##get rid of gridlines/grey background theme(plot.title = element_text(hjust = .5)) ## center title 3.1.5 Descriptive Plots of Multiple Variables Very often, we want graphs that help us visualize the trends in a continuous variable across different levels of a categorical variable. For instance, does agreeableness differ on average across people in different continents? Before plotting this in ggplot, we need to summarize our data. Since we’re interested in averages, we need to get the average agreeableness score within each continent. We can do this with a set of functions from the dplyr() package in R. group_by(): Organizes data by levels of a categorical variable summarize(): Performs some operation on your data %&gt;%: This is a special operator in dyplr called a “pipe”. It allows you to break up a set of actions on a dataset across multiple lines. When you see a pipe, you can read it as “and then” in the code: library(dplyr) ##create a new object called &quot;graphdata&quot; graphdata&lt;- big5 %&gt;% ##take our data.. AND THEN group_by(continent) %&gt;% ##Group that data by continent... AND THEN summarize(avg= mean(AgreeablenessSum)) ##Take the mean of the AgreeablenessSum data (which is grouped by continent) print(graphdata) ## # A tibble: 5 × 2 ## continent avg ## &lt;chr&gt; &lt;dbl&gt; ## 1 Africa 31.9 ## 2 Americas 31.4 ## 3 Asia 34.0 ## 4 Europe 31.3 ## 5 Oceania 31.8 We’ve created a new object in R that shows the levels of continent that we grouped the data by, as well as the mean Agreeableness scores for each continent group. We can now use ggplot() to graph the information in this data. Notice that in the initial ggplot function, the following has changed: For the data argument, I”m calling my new object “graphdata” instead of our original dataset “big5” The x argument “continent” references our column of groups in “graphdata” I’ve added a new argument called y in the aes() function. This specifies the variable we’d like to graph on our y axis. In this case, I want to graph the column of average Agreeableness stored in the “avg” column of our graphdata object I’ve added another new argument called fill in our aes() argument. Fill is an argument that colors the bars of the graph differently by levels of a group. In this case, I have filled the bars by the same variable that I’m graphing on the x axis (continent). This is a trick to get all the bars to be different colors! ggplot(data= graphdata, aes(x=continent, y=avg, fill=continent))+ geom_bar(stat=&quot;identity&quot;)+ ##geom that specifies I want a barplot ggtitle(&quot;Agreeableness Across Continents&quot;)+ ##title labs(x= &quot;Continents&quot;, y=&quot;Agreeableness&quot;)+ ##axis labels theme_classic()+ ##get rid of gridlines/grey background scale_fill_manual(values=natparks.pals(&quot;Yellowstone&quot;, 5))+ ##change the colors! theme(plot.title = element_text(hjust = .5)) ## center title Here is another example of plotting multiple variables. In the graph below, I’ve created a scatterplot between two continuous variables, Agreeableness and Extraversion, and used the color argument to color the individual data points by which continent that participant is from: ggplot(data=big5, aes(x=AgreeablenessSum, y=ExtraversionSum, color=continent))+ geom_point(alpha=.1)+ ##geom that specifies I want a scatterplot geom_jitter(width=1.5)+ ##helps get rid of overlapping points theme_classic()+ scale_color_manual(values=natparks.pals(&quot;Arches2&quot;, 5))+ ggtitle(&quot;The Relationship of Agreeableness and Extraversion&quot;)+ ##title labs(x= &quot;Agreeableness&quot;, y=&quot;Extraversion&quot;)+ ##axis labels theme(plot.title = element_text(hjust = .5)) ## center title Remember, these are just illustrations of the things you can do in ggplot2. Just because you can use lots of colors doesn’t necessarily mean you should :-). In this graph, color-coding by continent might make the graph more difficult to interpret. "],["ttests.html", "4 Two-Sample T-Tests 4.1 T-test Assumptions 4.2 Conducting and Interpreting the t-test", " 4 Two-Sample T-Tests Many of you will be interested in testing whether you have significant differences across the means of two groups. This question is best answered by a t-test. Let’s say I want to test whether right-handed folks (hand = 1) have a significant mean difference in Openness to Experience (OpennessSum) compared to those who are left handed (hand = 2). Our null hypothesis is the following: \\[H_0: \\mu_{Righthanded} = \\mu_{Lefthanded} \\] Our null hypothesis is always that there are no differences between the groups. Our mean of Openness for right-handed folks is the same as our mean for left-handed folks. Our alternative hypothesis is the following: \\[H_A: \\mu_{Righthanded} \\neq \\mu_{Lefthanded} \\] Since we have a nondirectional hypothesis, our alternative is that the means of the two groups are unequal, with no statement about which group might have a greater mean. Before we run the analyses, we need to make sure our data meet a certain set of assumptions. This is not an exhaustive list of the available techniques to check the assumptions for a t-test, but an illustration of some common technqiues. 4.1 T-test Assumptions The assumptions of a two-sample t-test are: Normality: Continuous outcome data should be normally distributed in each group Homogeneity of Variances/Equal Variances: The variances of each group are equal Independence of Groups: The two groups being tested are unrelated to each other (e.g., Group membership is mutually exclusive– one cannot belong to both groups) 4.1.1 Normality To examine normality, we can look at histograms of the outcome variable subsetted by our groups: par(mfrow = c(1,2)) ##plots graphs together hist(big5$OpennessSum[big5$hand == 2], #left = 2 main= &quot;LH Openness&quot;, xlab = &quot;&quot;) hist(big5$OpennessSum[big5$hand == 1], ##right = 1 main= &quot;RH Openness&quot;, xlab = &quot;&quot;) We can also look a graph called a Quantile-Quantile, or QQ-plot. QQ-plots graph your actual data against what your data should look like if it was normally distributed: ##Left-Handed qqnorm(big5$OpennessSum[big5$hand == 2], main = &quot;LH QQ Plot&quot;) qqline(big5$OpennessSum[big5$hand == 2]) ##Right-Handed qqnorm(big5$OpennessSum[big5$hand == 1], main = &quot;RH QQ Plot&quot;) qqline(big5$OpennessSum[big5$hand == 1]) Ideally, the points in a QQ-plot should hug the diagonal line as closely as possible. It is clear that the QQ-plot for left-handed folks has deviations from normality. Sometimes, visual examinations of normality might feel ambiguous. We can also use a quantitative assessment, such as a Shapiro-Wilks test: shapiro.test(big5$OpennessSum[big5$hand == 2]) ## ## Shapiro-Wilk normality test ## ## data: big5$OpennessSum[big5$hand == 2] ## W = 0.93615, p-value = 0.04741 shapiro.test(big5$OpennessSum[big5$hand == 1]) ## ## Shapiro-Wilk normality test ## ## data: big5$OpennessSum[big5$hand == 1] ## W = 0.98997, p-value = 0.003938 A significant result from Shapiro-Wilk normality test means that the distribution you’re testing significantly deviates from normality. Here, our distribution of our outcome across right-handed individuals is significant, p = .003. Since it’s nonnormal, we’ll try to address the issue using transformations. Our distribution of the outcome across left-handed individuals is p = .047, right on our p = .05 cutoff. Different researchers might make different decisions about whether or not to consider this normal. For this example, we will treat this group as normally distributed. For the right-handed group, we will try a series of data transformations to try and make the distribution more normal. While only three transformations are shown here, there are many to choose from. Three common trasnformations are to take the square root of the outcome variable, take the log of the outcome variable, and take the inverse of the outcome variable: hist(sqrt(big5$OpennessSum[big5$hand == 2]), main = &quot;LH Sqrt&quot;) hist(log(big5$OpennessSum[big5$hand == 2]), main = &quot;LH Log&quot;) hist(1/(big5$OpennessSum[big5$hand == 2]), main = &quot;LH Inverse&quot;) It looks like the transformations don’t quite make our distribution of left-handed folks normally distributed. Let’s check the sample sizes of our groups: big5 %&gt;% ##take our data... AND THEN group_by(hand) %&gt;% ##group by handedness... AND THEN summarize(samp = length(OpennessSum)) ##count the observations in each group ##and summarize in a table It looks like we have 34 left-handed people in our sample. While we won’t get into detail here, we are going to rely on a certain set of properties about sampling distributions of means, called the Central Limit Theorem (CLT). According to CLT, the sampling distribution of means will be normally distributed even if our data are not given a sufficient sample size (30 per group). Recall that we use estimates of the variability of the sampling distribution of means (called the standard error) to conduct the t-test. Since we have a sufficient sample size (n for each group is at least 30), we will proceed with our analysis. 4.1.2 Homogeneity of Variance (HOV) Homogeneity of Variances (HOV) is the assumption that the variances of the two groups are equal. We can look at the variances of the groups using var(): var(big5$OpennessSum[big5$hand == 2], na.rm = TRUE) ## [1] 10.77362 var(big5$OpennessSum[big5$hand == 1], na.rm = TRUE) ## [1] 16.18325 It looks like the variances for our left- and right-handed groups are unequal. In R, the default for the t-test analysis is called a “Welch’s t-test”. A Welch’s t-test is a test that is already corrected for violations of the HOV assumption. It is very common in psychological research to simply use this correction for t-tests by default, rather than formally assess whether data meet HOV requirements. That is our recommendation as well, and will be how we conduct the final analysis. 4.1.3 Independence of Groups For the two samples independent t-test, it is required that the groups are independent from each other. Technically, this means that the probability of an individual being selected into one group does not affect the probability of another individual to be selected into the other group. More plainly, we can think about this assumption in terms of mutual exclusivity– in our example, people who are right-handed cannot also be left-handed, and vice versa. This means we meet the assumption of independence of groups. Sometimes we are interested in mean comparisons across nonindependent (or paired) groups- for example, we might be want to compare pre- and post- test scores to see how much people have learned from a lecture or class. In this case, the groups are no longer independent. The individuals who took the pretest are the same individuals who took the posttest. Other examples of paired data might be comparing specific dyads– let’s say you wanted to compare differences in extraversion between mothers and their daughters. Because selection of a participant in the “mothers” group necessitates a specific individual into the “daughters” group, the data would be paired. You can run a version of a t-test for paired data by changing the paired argument in t.test to TRUE. 4.2 Conducting and Interpreting the t-test Before we conduct the final test, we have one issue– the hand variable in this dataset actually has three levels: right-handed (1), left-handed (2), and ambidextrous (3). Recall that a t-test can only be run with a single continuous outcome and a dichotomous (two-level) categorical predictor. If you’re interested in looking at more than 2 groups, review section 5 of this handout. The code below creates a new version of our original dataset (called tdat) with the ambidextrous cases removed: tdat &lt;- big5[big5$hand != 3,] Now, we can use that new data to run our analysis. The code below runs the same t-test twice. The first version of the code has some of the relevant default arguments explicit in the command: Use ?t.test to review all of the available arguments. Below is a list of arguments shown here, and what they do: var.equal = FALSE: Implements the Welch’s t-test correction for violation of HOV paired = FALSE: Dictates that we are running a two-sample, independent groups t-test. If there is a dependency between the two groups you wish to compare, change this argument to TRUE alternative = \"two.sided\": Runs a nondirectional, two-tailed test. ?t.test ##Explicit t.test(OpennessSum ~ hand, data=tdat, var.equal = FALSE, paired = FALSE, alternative = &quot;two.sided&quot;) ## ## Welch Two Sample t-test ## ## data: OpennessSum by hand ## t = -0.095859, df = 40.969, p-value = 0.9241 ## alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0 ## 95 percent confidence interval: ## -1.257291 1.143345 ## sample estimates: ## mean in group 1 mean in group 2 ## 33.06067 33.11765 ##Rely on Defaults t.test(OpennessSum ~ hand, data=tdat) ## ## Welch Two Sample t-test ## ## data: OpennessSum by hand ## t = -0.095859, df = 40.969, p-value = 0.9241 ## alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0 ## 95 percent confidence interval: ## -1.257291 1.143345 ## sample estimates: ## mean in group 1 mean in group 2 ## 33.06067 33.11765 The t-test output shows us our test statistic, our (corrected) degrees of freedom, the p-value associated with our test, a confidence interval, and the mean estimates for each group in our sample. Since the p-value associated with our t-test (p = 0.92) is above the .05 cutoff, we fail to reject the null hypothesis and cannot conclude there are differences in Openness between left- and right- handed groups. An example APA writeup is below: A Welch’s two-sample t-test suggests that there are no differences in Openness among left-handed (M = 33.11, SD = 3.28) and right-handed (M = 33.06, SD = 4.02) individuals, t(40.96) = -0.09, p = 0.92. ##get standard deviations for writeup sd(big5$OpennessSum[big5$hand == 2], na.rm = TRUE) ## [1] 3.282319 sd(big5$OpennessSum[big5$hand == 1], na.rm = TRUE) ## [1] 4.022841 Although it’s unusual in psychological research, you may want to test a directional hypothesis for your project. Let’s say our hypothesis is that left-handed people have higher Openness than right-handed people. Our t-test changes as such: t.test(OpennessSum ~ hand, data= tdat, alternative = &quot;less&quot;) ## ## Welch Two Sample t-test ## ## data: OpennessSum by hand ## t = -0.095859, df = 40.969, p-value = 0.462 ## alternative hypothesis: true difference in means between group 1 and group 2 is less than 0 ## 95 percent confidence interval: ## -Inf 0.9432432 ## sample estimates: ## mean in group 1 mean in group 2 ## 33.06067 33.11765 You may be wondering– if our hypothesis is that left-handed folks are MORE open that right-handed folks, why did we change our alternative argument to “less”? The default in R is to read groups in alphanumerical order. In this example, the right-handed group = 1 and the left-handed group = 2. You can think of R reading the above code as “group 1 has a lower mean than group 2”, which is equivalent to our hypothesis that states “group 2 has a higher mean than group 1.” If you do decide to use a directional test, make sure the direction aligns with how R is reading your groups! "],["anova.html", "5 Analysis of Variance (ANOVA) 5.1 Assumptions of ANOVA 5.2 Run an one-way ANOVA 5.3 Run a Post-hoc Analysis 5.4 Run a factorial ANOVA", " 5 Analysis of Variance (ANOVA) ANOVA is appropriate if you would like to test whether mean differences across three or more groups are significantly different from each other. This section will go over two types of between-subjects ANOVAs, called one-way ANOVA and factorial ANOVA. If you’re interested in doing a within-subjects analysis with repeated measures, we encourage you to review the section in this handout on multilevel modeling. Note: Regression and ANOVA are equivalent analyses, although its common to use ANOVA when looking at exclusively categorical predictors. The example used in this section of the handout for one-way ANOVA is equivalent to the example in the regression section. Throughout this section, we will run ANOVA using aov(), but they can also be run with lm(). We encourage you to review the regression section for interpretation of the lm() output. For our first example, let’s say we want to know if there are any significant mean differences in Openness across the groups in our continent variable. We call this analysis a one-way ANOVA, because we have a single categorical predictor (in this case, with five levels) and a single continuous outcome. Our null hypothesis is: \\[\\mu_{Americas} = \\mu_{Europe} = \\mu_{Asia} = \\mu_{Europe} = \\mu_{Oceania}\\] And our alternative hypothesis is at least one set of these means is not equal. The ANOVA informs us that there are significant mean differences among our groups, but does not provide information on where those group differences are (called an omnibus test). After running the analysis, we will use a follow up analysis called a post-hoc test to figure out which group comparisons are significantly different. 5.1 Assumptions of ANOVA You can think of the ANOVA as an extension of a t-test, and we recommend reviewing the section on t-test assumptions. We will discuss them again here and use many of the same techniques, with a focus on the residuals of the analysis. While there won’t be a technical explanation of what residuals are and why we want them to be normally distributed, we provide a practical overview of some graphs you can use to check ANOVA assumptions: 5.1.1 Normality of residuals Just like with t-tests, we want to examine the normality of our data. Although for t-tests we only examined normality of our raw data, we want to examine the normality of the residuals of our analysis. Below is a QQ-plot similar to those we constructed for t-tests, except this one plots the residuals of our analysis instead of a raw distribution of scores. The interpretation is the same– we want the points to hug the diagonal line as closely as possible. Here, we see some deviations at both tails of the plot. As with t-tests, some solutions may be to try data transformations on the variables– often, making the raw distributions more normal leads to a more normal residual distribution. We can also rely on properties of the CLT and say we have sufficient sample size to move forward. T-test and ANOVA are robust to normality violations, which means that they still return mostly accurate results even if our data aren’t quite normal. ##QQplot of residuals plot(lm(ExtraversionSum ~ continent, data=big5), which = 2) 5.1.2 Linearity of residuals Another assumption of these tests is that the relationships among variables are linear. The plot below is a residuals vs fitted plot. It plots the predictions (fitted values) your model made for each individual against the residuals (the differences between an individual’s actual value on Openness and what the ANOVA predicted that value to be). For linearity, we want to make sure the red line in this plot is straight and centered at 0. ##Residuals vs fitted plot plot(lm(OpennessSum ~ continent, data=big5), which = 1) 5.1.3 Homogeneity of variance (of residuals) We can also use the residuals vs fitted plot to assess whether the groups have equal variances. This time, we’re examining the vertical spread of the residuals around the red line. In this case, each group has similar spread. Ideally, the residuals vs fitted plot should look like a random cloud of points, with no clear patterns or clustering. The clusters you see in this plot are because we have a single categorical variable, so the model only makes five estimates (a mean estimate for each continent). Click here for more examples of what a residuals vs fitted plot should (and shouldn’t) look like. big5 %&gt;% group_by(continent)%&gt;% summarize(var = var(OpennessSum)) ##Residuals vs fitted plot plot(lm(OpennessSum ~ continent, data=big5), which = 1) 5.2 Run an one-way ANOVA The code below runs a one-way ANOVA. The structure of this code will be the same as it is for the t-test, where the formula is aov(outcome~grouping_variable) oneway_anova&lt;-aov(OpennessSum ~ continent, data=big5) summary(oneway_anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## continent 4 177 44.29 2.883 0.0222 * ## Residuals 495 7603 15.36 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If you see major issues with assumptions, there is code below for a few alternate options to run your analysis, such as a correction analogous to Welch’s correction for t-tests, the Huber White Robustness correction, and code for a nonparametric version of ANOVA. While the mechanics of these alternatives won’t be discussed here, we encourage you to learn more about these options if your project data has severe deviations from the assumptions of ANOVA. Choosing to use an ANOVA regardless of assumption violations can lead to untrustworthy results. library(car) # for Anova library(lmPerm) # for permutation test ##Issues with HOV- ANOVA with Welch&#39;s correction oneway.test(OpennessSum ~ continent, data = big5, var.equal = FALSE) ## ## One-way analysis of means (not assuming equal variances) ## ## data: OpennessSum and continent ## F = 3.1272, num df = 4.00, denom df = 247.32, p-value = 0.01557 ##Issues with normality and HOV ##Huber White Robust Correction oneway_anova&lt;-aov(OpennessSum ~ continent, data=big5) Anova(oneway_anova, white.adjust = TRUE) # notice for this we need to use the Anova() function, not anova() or aov() ## Coefficient covariances computed by hccm() ##Permutation test anova_perm&lt;- lmp(OpennessSum~continent, data = big5) ## [1] &quot;Settings: unique SS &quot; anova(anova_perm) ## Analysis of Variance Table ## ## Response: OpennessSum ## Df R Sum Sq R Mean Sq Iter Pr(Prob) ## continent1 4 177.1 44.287 5000 0.0026 ** ## Residuals 495 7603.0 15.360 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.3 Run a Post-hoc Analysis Note that a post-hoc analysis is only useful if the omnibus ANOVA returns a significant result. If you run a post-hoc analysis on a nonsignificant ANOVA, any pairwise comparisons from a post-hoc analysis will also be nonsignificant. Since our omnibus ANOVA is significant, we know that there is a significant mean difference among at least one pair of five continents. The simplest way to look at the pairwise comparisons of each group would be to use a series of t-tests examining the groups. However, when we do multiple statistical tests, we inflate our chances for type 1 error (a false positive result). Using an appropriate post-hoc test allows us look at pairwise comparisons while controlling our type one error rate. The code below runs a Tukey’s Honestly Significant Difference test (more commonly refered to as a Tukey test). This is a type of post-hoc test that will return every possible pairwise comparison in your data with signficance values that have been corrected for multiple testing. You can interpret the p-values from this output the same way you would interpret significance for a t-test, with a p &lt; .05 cutoff. TukeyHSD(oneway_anova) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = OpennessSum ~ continent, data = big5) ## ## $continent ## diff lwr upr p adj ## Americas-Africa -0.16 -1.6774539 1.3574539 0.9984808 ## Asia-Africa -1.49 -3.0074539 0.0274539 0.0571107 ## Europe-Africa -0.02 -1.5374539 1.4974539 0.9999996 ## Oceania-Africa 0.11 -1.4074539 1.6274539 0.9996535 ## Asia-Americas -1.33 -2.8474539 0.1874539 0.1170761 ## Europe-Americas 0.14 -1.3774539 1.6574539 0.9991013 ## Oceania-Americas 0.27 -1.2474539 1.7874539 0.9885310 ## Europe-Asia 1.47 -0.0474539 2.9874539 0.0628077 ## Oceania-Asia 1.60 0.0825461 3.1174539 0.0329635 ## Oceania-Europe 0.13 -1.3874539 1.6474539 0.9993291 We can see here that only one pairwise comparison meets our p &lt; .05 cutoff. Oceania and Asia have a significant mean difference of 1.60 on Openness (this means that the first group, Oceania, has a mean that is 1.6 higher than average Openness for Asia). Below is an example writeup for a one-way ANOVA and Tukey test post-hoc analysis. In order for your writeup, you will need information from your aov() summary, your Tukey test results, and some descriptive statistics: big5 %&gt;% group_by(continent)%&gt;% summarize(mean= mean(OpennessSum), sd = sd(OpennessSum), n = n()) summary(oneway_anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## continent 4 177 44.29 2.883 0.0222 * ## Residuals 495 7603 15.36 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 TukeyHSD(oneway_anova) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = OpennessSum ~ continent, data = big5) ## ## $continent ## diff lwr upr p adj ## Americas-Africa -0.16 -1.6774539 1.3574539 0.9984808 ## Asia-Africa -1.49 -3.0074539 0.0274539 0.0571107 ## Europe-Africa -0.02 -1.5374539 1.4974539 0.9999996 ## Oceania-Africa 0.11 -1.4074539 1.6274539 0.9996535 ## Asia-Americas -1.33 -2.8474539 0.1874539 0.1170761 ## Europe-Americas 0.14 -1.3774539 1.6574539 0.9991013 ## Oceania-Americas 0.27 -1.2474539 1.7874539 0.9885310 ## Europe-Asia 1.47 -0.0474539 2.9874539 0.0628077 ## Oceania-Asia 1.60 0.0825461 3.1174539 0.0329635 ## Oceania-Europe 0.13 -1.3874539 1.6474539 0.9993291 A one-way ANOVA examining mean differences in Openness across continents returned a significant result, F(4,495) = 2.88, p = .02. A post-hoc Tukey test suggests a significant mean difference in Openness between Oceania (M = 33.5,SD = 3.78) and Asia (M = 31.9,SD = 3.69), p = .03. All other pairwise comparisons were nonsignficant. Note: Just because results are not significant does not mean they’re not worth reporting! With a large amount of results like in this analysis, it is much easier to include a table rather than write everything out in the writeup. Another popular test for multiple comparisons is the Bonferroni correction, which creates a new p-value cutoff by dividing the normal p value cutoff (.05) by the number of tests you want to run. In the example above, we ran 10 pairwise comparisons. Let’s calculate a new cutoff using Bonferroni’s criterion. .05/10 ## [1] 0.005 If we chose to use Bonferroni’s correction instead of a Tukey test, we would not say a result is significant unless it meets a new cutoff of p &lt; .005. While it’s easy to implement and useful to know, many consider the Bonferroni correction to be too stringent. These are just two examples of possible post-hoc tests you can use to interpret the results of an ANOVA. Some may be more appropriate than others based on your research question, and some may be more widely used in some subfields compared to others. We won’t exhaust post-hoc tests here, but make sure you’re using something to correct for multiple comparisons! 5.4 Run a factorial ANOVA Many of you will be interested in running an ANOVA with multiple categorical variables, an analysis we call factorial ANOVA. We refer to the design of a factorial ANOVA by the number of predictor variables we have, and the number of levels within each variable. In the example below, we are going to predict average neuroticism from two predictor variables, continent and gender. Since continent has 5 levels (Oceania, Asia, Americas, Europe, Africa) and gender has three levels (Male, Female, Other), we would call this analysis a 5x3 factorial ANOVA. In factorial designs, we are often interested in the interaction between our two predictor variables. That is, we’re not just interested in the impact of continent and gender separately on average neuroticism, but also whether the two groups are dependent on each other. For instance, maybe being from America predicts higher average neuroticism, but only for males. Interaction terms help us understand these nuances about how our predictor variables affect each other. For more information on interactions, review the Multiple Regression section of the handout. The code for the analysis is below. In the formula for aov(), we’ve included the gender variable separated from the continent variable by an asterisk (*). This is shorthand to tell R to include both variables in the analysis, as well as their interaction term. twoway_anova&lt;-aov(NeuroSum ~ continent*gender, data=big5) summary(twoway_anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## continent 4 513 128.18 2.738 0.02826 * ## gender 1 288 288.40 6.160 0.01340 * ## continent:gender 4 632 157.88 3.373 0.00975 ** ## Residuals 489 22892 46.81 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 1 observation deleted due to missingness ##alternative but identical formula specification: example2&lt;-aov(NeuroSum ~ continent+gender+continent:gender, data=big5) summary(example2) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## continent 4 513 128.18 2.738 0.02826 * ## gender 1 288 288.40 6.160 0.01340 * ## continent:gender 4 632 157.88 3.373 0.00975 ** ## Residuals 489 22892 46.81 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 1 observation deleted due to missingness We see from our results that we have three significant effects– the main effect of continent is significant, the main effect of gender is significant, and the interaction term is significant. IMPORTANT: When we have a significant interaction term, we do not want to interpret the main effects of our analysis, because they will always misrepresent some aspect of our results. Let’s visualize the interaction and see what happens if we try to interpret main effects. ##Rename gender variable levels for graph legend big5$Gender&lt;- as.factor(big5$gender) levels(big5$Gender) &lt;- c(&quot;Male&quot;,&quot;Female&quot;,&quot;Other&quot;) ##summarize means for plot plotdat&lt;- big5%&gt;% group_by(continent, Gender) %&gt;% summarise(groupm = mean(NeuroSum)) ##graph means ggplot(data=subset(plotdat, !is.na(Gender)), aes(x = continent, y = groupm, color = Gender))+ geom_line(aes(group = Gender)) + geom_point()+ ggtitle(&quot;Neuroticism Across Continent and Gender&quot;)+ ##title labs(x= &quot;Continent&quot;, y=&quot;Average Neuroticism&quot;)+ theme_classic() Here is average neuroticism of both groups plotted together on a single line graph. From here, it becomes clear that the interpretations of the main effects of our analysis become inaccurate when there is an interaction term. When we interpret a main effect, we do not take into account any other groups in the analysis. Let’s look at the means across the male and female levels of our gender variable: mean(big5$NeuroSum[big5$gender == 1], na.rm = TRUE) ## [1] 30.28788 mean(big5$NeuroSum[big5$gender == 2], na.rm = TRUE) ## [1] 31.80201 If we interpret the main effect of gender on neuroticism, we might conclude that female participants have higher average neuroticism than males. However, if we look at our interaction graph, that is not always correct! Men have higher neuroticism scores in the Americas. That is to say, the effect of gender on average neuroticism DEPENDS on the continent variable. 5.4.1 Simple Main Effects Analysis Disclaimer: Simple main effects analysis requires recalculating some values from different portions of output. Some basic code is provided here, but we recommend reviewing the mechanics of ANOVA (e.g., the different sums of squares values, what the F ratio is) if you choose to conduct these! To assess a significant interaction, we can follow up with a simple main effects analysis, where we test one variable at each level of the other variable. In the code below, I subset the dataset to each of the five levels of continent, then run one-way anovas predicting average neuroticism from gender. ##Create five data subsets Amer &lt;- big5[big5$continent == &quot;Americas&quot;,] Euro &lt;- big5[big5$continent == &quot;Europe&quot;,] Ocean &lt;- big5[big5$continent == &quot;Oceania&quot;,] Afr &lt;- big5[big5$continent == &quot;Africa&quot;,] Asia &lt;- big5[big5$continent == &quot;Asia&quot;,] ##Run one-way anovas for each subset SME1&lt;- lm(NeuroSum ~ gender, data = Amer) SME2&lt;- lm(NeuroSum ~ gender, data = Euro) SME3&lt;- lm(NeuroSum ~ gender, data = Ocean) SME4&lt;- lm(NeuroSum ~ gender, data = Afr) SME5&lt;- lm(NeuroSum ~ gender, data = Asia) summary(SME1) ## ## Call: ## lm(formula = NeuroSum ~ gender, data = Amer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.505 -5.275 -0.045 4.955 17.955 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.964 2.147 16.286 &lt;2e-16 *** ## gender -2.459 1.250 -1.967 0.052 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.485 on 97 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.03837, Adjusted R-squared: 0.02846 ## F-statistic: 3.871 on 1 and 97 DF, p-value: 0.05199 summary(SME2) ## ## Call: ## lm(formula = NeuroSum ~ gender, data = Euro) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.9677 -4.8143 0.6346 5.0323 13.0323 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 28.559 2.285 12.496 &lt;2e-16 *** ## gender 2.205 1.351 1.631 0.106 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.56 on 98 degrees of freedom ## Multiple R-squared: 0.02644, Adjusted R-squared: 0.0165 ## F-statistic: 2.661 on 1 and 98 DF, p-value: 0.106 summary(SME3) ## ## Call: ## lm(formula = NeuroSum ~ gender, data = Ocean) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.306 -5.737 -1.306 5.263 15.263 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 26.167 2.411 10.854 &lt;2e-16 *** ## gender 2.570 1.426 1.802 0.0745 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.92 on 98 degrees of freedom ## Multiple R-squared: 0.03209, Adjusted R-squared: 0.02221 ## F-statistic: 3.249 on 1 and 98 DF, p-value: 0.07455 summary(SME4) ## ## Call: ## lm(formula = NeuroSum ~ gender, data = Afr) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.9714 -6.4769 -0.2242 5.0286 16.5231 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.466 2.758 9.958 &lt;2e-16 *** ## gender 1.505 1.606 0.938 0.351 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.659 on 98 degrees of freedom ## Multiple R-squared: 0.008889, Adjusted R-squared: -0.001224 ## F-statistic: 0.8789 on 1 and 98 DF, p-value: 0.3508 summary(SME5) ## ## Call: ## lm(formula = NeuroSum ~ gender, data = Asia) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.480 -4.480 -0.480 4.765 16.520 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 26.461 2.014 13.137 &lt; 2e-16 *** ## gender 4.019 1.254 3.205 0.00182 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.511 on 98 degrees of freedom ## Multiple R-squared: 0.09488, Adjusted R-squared: 0.08564 ## F-statistic: 10.27 on 1 and 98 DF, p-value: 0.001823 For each simple main effects analysis, we recalculate our F statistic and p value using a value from our original ANOVA with the interaction, called the mean squares residual. Using the SME for gender @ Asia, I refit our ANOVA with the lm() function and pull out the information I need to recalculate F: twoway_reg&lt;-lm(NeuroSum ~ continent*gender, data=big5) ##our original analysis MSres&lt;-anova(twoway_reg)[[&quot;Mean Sq&quot;]][4] ##MS residual from factorial ANOVA MSgender&lt;-anova(SME5)[[&quot;Mean Sq&quot;]][1] ##MS groups from SME one-way ANOVA Our F statistic is calculated by dividing the mean squares value from the SME analysis by the mean squares residual from the factorial ANOVA. Then, we use the pf() function to calculate our new F statistic’s corresponding p value: F&lt;-MSgender/MSres F ## [1] 9.303278 pf(F,df1=1, df2=487, lower.tail = FALSE) ## [1] 0.002412113 Below is an example writeup for a factorial ANOVA with a significant interaction term: A 5 (Continent: Americas, Asia, Africa, Europe, and Oceania) x 3 (Gender: Male, Female, Other) factorial ANOVA suggests a significant effect of continent (F(4, 487) = 2.73, p = .02), gender (F(2, 487) = 3.80, p = .02), and their interaction (F(5,487) = 2.54, p = .02).A simple main effects analysis found significant gender effects for Asia(F(1,487) = 9.3, p = .002), and no gender differences across all other continents. Note: It would be best to present all results, even nonsignificant ones, in a table! Note II: You may have noticed that the simple main effects analysis tells us that there are significant gender differences for Asia, but not what those gender differences are. You can follow up an SME even further with another pairwise comparison. Remember that, unless corrected for multiple comparisons, using several statistical tests will increase your chance of getting false positive results. Use something like the Bonferroni correction (see #ttests post-hoc section) in order to control your type 1 error rate. "],["correlation.html", "6 Correlation and Tests of Correlations 6.1 Estimating Correlations 6.2 Tests for Correlations 6.3 Visualizing Correlations", " 6 Correlation and Tests of Correlations library(ggplot2) # for general data visualization library(rstatix) # correlation test of multiple pairs at once library(GGally) # for pairwise scatterplots library(ggcorrplot) # for heat map visualization Correlation allows us to understand the level of linear relation between two continuous variables. It ranges from -1 to 1, where negative values indicate negative relations (as one variable increases, the other variable decreases) and positive values indicate positive relations (as one variable increases, the other variable also increases). Correlations closer to -1 or 1 indicate stronger relations, whereas correlations closer to 0 indicate weaker relations. Some important things to note about correlations: Correlations represent linear relations. This means that your variables may have a non-linear relation, but it is not captured by the correlation (check out [Anscombe’s Quartet[(https://en.wikipedia.org/wiki/Anscombe%27s_quartet)] for an example of this). It’s important to plot your data to see! Correlations represent relations between continuous variables. If you have ordinal data with more than 5 levels, it is generally ok to still estimate correlations with this. Correlation is a non-directional relation: You are only looking at how one variable changes as the other variable goes up or down. You are not able to see how one variable predicts the other. 6.1 Estimating Correlations Suppose that we were interested in the relation between age and each of the Big 5 traits. First, as I mentioned, it is important to plot your data before calculating a correlation, to ensure the relations are roughly linear. Let us start by focusing on the relation between age and Conscientiousness. plot(x = big5$age, big5$ConscSum, xlab = &quot;Age&quot;, ylab = &quot;Conscientiousness&quot;, main = &quot;Plot of Age versus Conscientiousness&quot;) Now that we know the relation is linear enough, we can calculate a correlation. To estimate a correlation, we use the cor function. cor(x = big5$age, y = big5$ConscSum) ## [1] -0.0518399 This function takes two arguments: x and y, which refer to the two variables you want to find the correlation between. If you don’t want to use the Pearson correlation, and instead want to use the Spearman correlation (good if your data are non-normal or ordinal) or Kendall correlation, you can specify this using the method argument. The output is the estimated Pearson correlation. In this case, that is -.05, which means that are two variables are weakly negatively related. In other words, as age increases, people’s Conscientiousness decreases, but this relation is almost non-existent (which is not too surprising based on the plot). However, we’re often not just estimating one correlation, but multiple. It would be extremely tedious (and error-prone) to copy-and-paste the cor function over and over again. Luckily, we can get what is known as a correlation matrix by simply giving the cor function a dataframe, and it will estimate all pairwise correlations. To plot all pairwise combinations of variables at once, we can use a few different functions: 6.1.1 Option 1: Base R Using the pairs function in base R will get you the scatterplots of each pairwise combination of variables (e.g., for variables X, Y, and Z, this will plot X versus Y, X versus Z, and Y versus Z) but not much else. The arguments of the pairs function is mainly the dataframe you want to plot, as well as (optionally) the labels for your variable names. pairs(x = big5[,c(&quot;age&quot;, &quot;OpennessSum&quot;, &quot;ExtraversionSum&quot;, &quot;ConscSum&quot;, &quot;AgreeablenessSum&quot;, &quot;NeuroSum&quot;)], labels = c(&quot;Age&quot;, &quot;Openness&quot;, &quot;Extraversion&quot;, &quot;Conscientiousness&quot;, &quot;Agreeableness&quot;, &quot;Neuroticism&quot;)) 6.1.2 Option 2: ggplot If we wanted some more information (e.g., the distribution of each variable on its own, and the estimated correlation), we could use the ggpairs function, which is from the GGally package, but works with ggplot to create a prettier plot. Just like the pairs function, all you need to give ggpairs is the dataframe with the variables you want to make pairwise scatterplots from. You can also add labels using the columnLabels argument. ggpairs(big5[,c(&quot;age&quot;, &quot;OpennessSum&quot;, &quot;ExtraversionSum&quot;, &quot;ConscSum&quot;, &quot;AgreeablenessSum&quot;, &quot;NeuroSum&quot;)], columnLabels = c(&quot;Age&quot;, &quot;Openness&quot;, &quot;Extraversion&quot;, &quot;Conscientiousness&quot;, &quot;Agreeableness&quot;, &quot;Neuroticism&quot;)) Now you can see we have a lot more information: In addition to the same scatterplots we had before, we now have the distribution of each individual variable (so we can see if it’s normally distributed or skewed), plus an estimate of the correlation between each variable pair (stars indicate whether that correlation is statistically significant from 0 or not). Once we’ve glanced at our data plot and assessed whether correlations between these variables make sense, we can calculate the correlation matrix (I wrapped this in a round statement so the final result is easier to read): round(cor(big5[, c(&quot;age&quot;, &quot;OpennessSum&quot;, &quot;ExtraversionSum&quot;, &quot;ConscSum&quot;, &quot;AgreeablenessSum&quot;, &quot;NeuroSum&quot;)], use = &quot;everything&quot;), 2) ## age OpennessSum ExtraversionSum ConscSum AgreeablenessSum ## age 1.00 0.03 -0.01 -0.05 -0.02 ## OpennessSum 0.03 1.00 0.05 0.28 0.11 ## ExtraversionSum -0.01 0.05 1.00 0.17 0.22 ## ConscSum -0.05 0.28 0.17 1.00 0.17 ## AgreeablenessSum -0.02 0.11 0.22 0.17 1.00 ## NeuroSum -0.21 0.13 -0.07 0.17 0.23 ## NeuroSum ## age -0.21 ## OpennessSum 0.13 ## ExtraversionSum -0.07 ## ConscSum 0.17 ## AgreeablenessSum 0.23 ## NeuroSum 1.00 Notice that instead of giving an x and y, I just gave one argument, which is the dataset (of the specific columns I want). This tells R to calculate all possible correlations between the columns in that dataset. Since I am now working with different columns, which might have different patterns of missingness, it is also important to specify the “use” argument: “everything” or “all.obs” will use all possible observations, “complete.obs” will only use participants who have complete data on all variables, and “pairwise.complete.obs” will calculate correlations for each pair of variables using participants who have complete data on that pair (this means that different people may be used to calculate each correlation). Examining the correlation matrix, we can see the diagonal elements are all 1. This makes sense: a variable is perfectly related to itself. The more interesting part is the off-diagonal elements: looking at these, we can see most correlations range from weak to medium, and the highest correlation is 0.28 (between Openness and Conscientiousness). Also, the values above the diagonal are the same as those below the diagonal - this is because the correlation between X and Y is the same as the correlation between Y and X. 6.2 Tests for Correlations Although we were able to estimate the correlations and eyeball whether they are weak, medium, or strong, we are more likely to test whether or not a correlation is equal to zero or not. This is because, in real data, no correlation will be exactly equal to 0: it may be very small, but still non-zero. Conversely, a correlation may seem to be of medium strength, but there is so much sampling variability present that we cannot say the correlation is significantly different from 0. To test whether correlations are significantly different from 0, we use the cor.test function. This function takes the same arguments as the cor function we used earlier. cor.test(big5$age, big5$ConscSum) ## ## Pearson&#39;s product-moment correlation ## ## data: big5$age and big5$ConscSum ## t = -1.1584, df = 498, p-value = 0.2473 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.13889907 0.03601441 ## sample estimates: ## cor ## -0.0518399 You see that we get the same estimate of the correlation we got before (-.05), but now we have additional information: a t-statistic, the associated df, the p-value for the hypothesis test, and the 95% confidence interval. In this case, our p-value is .25. Since this is larger than the conventional alpha of .05, we fail to reject the null: there is not enough evidence that this correlation between age and conscietiousness is significantly different from 0. What about all the other correlations? The cor.test function will not work here - you can only calculate a correlation between one pair of variables t a time. Luckily, there is another function in the rstatix package, called cor_test (that’s not confusing at all). cor_test(big5, vars = c(&quot;age&quot;, &quot;OpennessSum&quot;, &quot;ExtraversionSum&quot;, &quot;ConscSum&quot;, &quot;AgreeablenessSum&quot;, &quot;NeuroSum&quot;), vars2 = c(&quot;age&quot;, &quot;OpennessSum&quot;, &quot;ExtraversionSum&quot;, &quot;ConscSum&quot;, &quot;AgreeablenessSum&quot;, &quot;NeuroSum&quot;), use = &quot;everything&quot;) This function returns an estimate of the correlation, the test statistic, the p-value, and confidence interval for each pair of variables. 6.3 Visualizing Correlations Although scatterplots made using the pairs or ggpairs functions are common ways to visualize the associations between variables, another popular approach is to create a heat map or correlelogram. These allow us to take advantage of the fact that people perceive color/size pretty quickly, so it uses different shades of a color or different sizes of some shape to indicate how strong a correlation between two variables is. Heat maps use just color to differentiate the strength of correlations. To create a heat map, there are a few different packages and functions you can use (including ggcor in the GGally package we used earlier). Another option is the ggcorrplot package, which contains the ggcorrplot function. With the ggcorrplot function, you need to provide the correlation matrix of your dataset to the corr argument. However, this will plot the entire correlation matrix, even though the entries above and below the diagonal contain redundant information. So I recommend setting another argument, type, to “lower” (only plot correlations below the diagonal) or “upper” (only plot correlations above the diagonal). cor_matrix = cor(big5[, c(&quot;age&quot;, &quot;OpennessSum&quot;, &quot;ExtraversionSum&quot;, &quot;ConscSum&quot;, &quot;AgreeablenessSum&quot;, &quot;NeuroSum&quot;)], use = &quot;everything&quot;) # In the heat map, the names of your variables will be taken from the correlation matrix - therefore, I am renaming them below just to make them nicer to read rownames(cor_matrix) = colnames(cor_matrix) = c(&quot;Age&quot;, &quot;Openness&quot;, &quot;Extraversion&quot;, &quot;Conscientiousness&quot;, &quot;Agreeableness&quot;, &quot;Neuroticism&quot;) # make the heat map ggcorrplot(corr = cor_matrix, type = &quot;lower&quot;, outline.color = &quot;black&quot;) With this heat map, reddish values correspond to positive correlations, while bluish colors correspond to negative correlations. The darker the color, the stronger (closer to -1 or 1) the correlation is. This allows us to, at a glance, assess the relations between our variables. [As a side note, you can change the colors if you prefer, by providing 3 colors (for negative, zero, and positive correlation values) to the colors argument in the ggcorrplot function.] However, we have now lost information on the exact values of the estimated correlations. If we want that back, we can add another argument to the ggcorrplot function: lab = TRUE, which places the correlation coefficient on top of the plot. ggcorrplot(corr = cor_matrix, type = &quot;lower&quot;, outline.color = &quot;black&quot;, lab = TRUE, lab_size = 3, lab_col = &quot;black&quot;) # lab_size: How big do you want the labeled correlation coefficients to be # lab_color: What color do you want the text for the correlation coefficients to be If we wanted to use shape as well as color to help us evaluate our correlations, we could plot a correlelogram. Luckily, this can easily be done by setting method = “circle” in the ggcorrplot function. ggcorrplot(corr = cor_matrix, type = &quot;lower&quot;, method = &quot;circle&quot;, outline.color = &quot;black&quot;, lab = TRUE, lab_size = 2, lab_col = &quot;black&quot;) Now, not only do darker or lighter colors tell us the strength of the correlation coefficient, but bigger circles correspond to stronger correlation coefficients as well! "],["regression.html", "7 Simple and Multiple Linear Regression 7.1 Simple Regression 7.2 Multiple Regression 7.3 Visualizing Regression Lines 7.4 Checking Assumptions of the Regression Model", " 7 Simple and Multiple Linear Regression library(ggplot2) # for general data visualization library(skedastic) # for testing model assumptions Although correlations can help us evaluate the association between a pair of variables, this association is not directional: either X can lead to Y or Y can lead to X. But most of the time we are interested in directional relations between variables, either because we think it can help us uncover possible causal associations, or because the relation only makes sense in a particular direction (e.g., age is likely to affect how extraverted someone is, but how extraverted someone is will not affect their age). To answer questions where we have one (or more) variables predicting a continuous outcome, we can use regression. Some of the terminology I will be using below is: Independent variable / predictor variable / explanatory variable: The variable that you think affects or explains your outcome variable Dependent variable / outcome variable: The variable that you want to try and predict or explain 7.1 Simple Regression Simple regression is used when we have one independent variable predicting the outcome variable. We will start off with a continuous independent variable; a bit further down the page, we will talk about what to do when you have categorical variables (with 2 or more categories). 7.1.1 Simple Regression with a Continuous Predictor One of the most straightforward cases of simple regression is when both your independent and dependent variables are continuous. For an example, let us say we wanted to predict Neuroticism using a person’s age. All linear regression models use the same code: lm(Dep.Variable ~ Indep.Variable, data = dataset.name) As you have more than one independent variable (multiple regression) you just separate them using the + sign. So, if we wanted to predict Neuroticism (our dependent variable) using age (our independent variable), we could just write: lm(NeuroSum ~ age, data = big5) ## ## Call: ## lm(formula = NeuroSum ~ age, data = big5) ## ## Coefficients: ## (Intercept) age ## 34.5249 -0.1254 However, this gives us no information except for the estimates of our model intercept and our slope. Therefore, it is usually better to save your model as an object. Then you can use functions like summary to access not only the estimates of those coefficients, but also the associated test statistics and p-values. model1 = lm(NeuroSum ~ age, data = big5) summary(model1) ## ## Call: ## lm(formula = NeuroSum ~ age, data = big5) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.1376 -5.1411 -0.0165 4.8568 16.6089 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.52491 0.75791 45.553 &lt; 2e-16 *** ## age -0.12535 0.02594 -4.832 1.81e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.834 on 498 degrees of freedom ## Multiple R-squared: 0.04478, Adjusted R-squared: 0.04286 ## F-statistic: 23.34 on 1 and 498 DF, p-value: 1.806e-06 Now we have a lot more information! What does this tell us? Intercept: The intercept (34.52) is the expected value of our outcome (Neuroticism) when our independent variable (age) is 0. Slope for age: The slope representing the effect of age on Neuroticism is -0.12. This means for that every 1-year increase in a person’s age, their Neuroticism score is expected to decrease by 0.12 points. This value is significantly different from 0, indicating that age does have a relation with Neuroticism. Another value of interest from this model output is the \\(R^2\\) value at the bottom of the summary. You’ll notice that there are two values - Multiple R-Squared and Adjusted R-Squared. Although the two are pretty similar in this example, Adjusted R-Squared accounts for the fact that as you add more and more predictors to your model, \\(R^2\\) will always increase, and so can be preferred. \\(R^2\\) tells you what proportion of variance in your outcome variable is explained by the predictors in your model. So in this case, about \\(4.3\\%\\) of the variance in Neuroticism is explained by people’s age. If we wanted to visualize this relation, we could use ggplot. ggplot(data = big5, aes(x = age, y = NeuroSum))+ geom_point()+ # plot the data points geom_smooth(method = &quot;lm&quot;, se = TRUE)+ # adds in the estimated regression line (along with the standard error of prediction to show how uncertain we are) theme_classic()+ # removes some of the background labs(x = &quot;Age&quot;, y = &quot;Neuroticism&quot;, title = &quot;Plot of Age versus Neuroticism&quot;) # add plot labels so people seeing this plot on its own know what it&#39;s about! ## `geom_smooth()` using formula = &#39;y ~ x&#39; 7.1.2 Simple Regression with a Centered Continuous Predictor One thing you may have noticed is that the intercept in our previous model was pretty meaningless. A Neuroticism score for someone who is 0 years old?! Therefore, you’ll often find people centering their predictor variables at more meaningful values, such as the mean of that variable. This keeps the relation between your predictor and outcome unchanged, but puts the intercept somewhere more meaningful to interpret. To center your predictor, all you have to do is subtract the mean of your independent variable from every value of the independent variable. You can save this as a new column in your dataset, and then use that as a predictor in your regression model like we did above. # create centered predictor variable as a new column big5$age_centered = big5$age - mean(big5$age, na.rm = TRUE) # run the regression model centered_model = lm(NeuroSum ~ age_centered, data = big5) summary(centered_model) ## ## Call: ## lm(formula = NeuroSum ~ age_centered, data = big5) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.1376 -5.1411 -0.0165 4.8568 16.6089 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 31.17400 0.30565 101.994 &lt; 2e-16 *** ## age_centered -0.12535 0.02594 -4.832 1.81e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.834 on 498 degrees of freedom ## Multiple R-squared: 0.04478, Adjusted R-squared: 0.04286 ## F-statistic: 23.34 on 1 and 498 DF, p-value: 1.806e-06 The interpretation of these parameters is very similar to before - but which parameters changed their value? Intercept: The intercept (31.17) is the expected value of our outcome (Neuroticism) when our independent variable (age_centered) is 0. In this case however, age_centered (\\(Age - \\bar{Age}\\)) is only 0 when \\(Age = \\bar{Age}\\), so our intercept represents the expected value of Neuroticism for someone who is the average age in our dataset (which is 26.73). This value has changed from our initial model, because we have changed at which value of age we interpret our intercept. Slope for age: The slope representing the effect of age_centered on Neuroticism is -0.12. This means for that every 1-year increase in a person’s age, their Neuroticism score is expected to decrease by 0.12 points. This value did not change from our initial analysis, because centering our predictor variable is a linear transformation, and does not change the relation between our predictor and outcome variable. 7.1.3 Simple Regression with a Binary Predictor Although regression is typically conducted with a continuous predictor, it can also handle binary or categorical predictor variables. We will first walk through the interpretation of the regression model when the predictor is binary (only 2 categories), and then go on to when the predictor is categorical (&gt;2 categories). In the case of a simple regression model with a binary predictor variable, this is the same as conducting a t-test to examine whether there are group differences in the mean of our continuous variable! To run a linear regression with a binary variable, first make sure your binary variable is saved as a factor in R. Suppose we wanted to examine whether there were differences in average levels of Openness for people who were left versus right-handed (remember this from our #ttests section?). class(big5$hand) # currently, the class of our handedness variable is an integer; in fact it has the values 0 (didn&#39;t answer the question), 1 (right-handed), 2 (left-handed), 3 (both) ## [1] &quot;integer&quot; # since I just want to compare left- and right-handed participants, I will just create a subset of the data with only those participants for this example big5_subset = big5[big5$hand == 1 | big5$hand == 2, ] # Now, I will make hand in this subsetted data frame a factor variable big5_subset$hand = factor(big5_subset$hand) head(big5_subset$hand) # you will notice that the levels of hand are still 1 and 2 - if we wanted to, we could rename these to be something more meaningful (e.g., right/left), but for now we&#39;ll keep it as is ## [1] 1 1 1 1 1 1 ## Levels: 1 2 Once our binary variable is saved as a factor, we can run a regression model just as we have been doing. model2 = lm(OpennessSum ~ hand, data = big5_subset) summary(model2) ## ## Call: ## lm(formula = OpennessSum ~ hand, data = big5_subset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.0607 -2.0607 -0.0607 2.9393 12.9393 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 33.06067 0.18848 175.404 &lt;2e-16 *** ## hand2 0.05697 0.70746 0.081 0.936 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.976 on 477 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 1.36e-05, Adjusted R-squared: -0.002083 ## F-statistic: 0.006485 on 1 and 477 DF, p-value: 0.9358 Ok, so how can we interpret each of these regression parameters? (Intercept): The intercept of 33.06 is the expected Openness score when our predictor variable (hand) is 0. But how can this be - our predictor variable only has 2 options - 1 or 2! When you use a categorical variable as a predictor in a regression model, R is doing some stuff behind-the-scenes to make the intercept interpretable. It is assigning a value of 0 to the first category in our predictor variable - unless you’ve specified otherwise, the first category is whichever value comes first alphanumerically. This is called the reference group. So in this case, since the two options of our binary variable are 1 and 2, and 1 comes first numerically, everyone who was in that group (everyone who is right-handed) gets assigned a 0 for their group value behind-the-scenes. The intercept of 33.06 is actually the expected value of Openness when a person is right-handed. This expected value is just the average of the group, so 33.06 is the average Openness score for right-handed people in our sample. When your predictor is binary, the intercept represents the average value of the “reference group”, or the group whose value on the binary variable comes first alphanumerically. hand2: What does hand2 mean? Well, hand is the name of our predictor variable, and the 2 is indicating which group this slope is representing (this will become more important when we have more than 2 groups). When you have categorical variables, the slope represents the difference between the mean of the reference group and the mean of the group for that slope - so left-handed people have an average Openness score that is 0.06 points higher than right-handed people (which means their Openness score is \\(33.06 + 0.06 = 33.12\\)). This slope is not significant - this means that the difference in means is not significantly different from 0, or right-handed and left-handed people have Openness scores that are not significantly different from each other. This matches the results of the t-test we conducted earlier - in fact, the t-value (0.08) is roughly same as the test statistic we calculated earlier (with maybe a difference in sign, due to a difference in the order the test statistic is calculated). This is why a simple linear regression with binary variable is the same as a two-sample t-test: because a test of the slope is the same as testing the difference in the two means! 7.1.4 Simple Regression with a Categorical Predictor (&gt; 2 categories) We can apply the same strategy when we have a categorical variable with more than 2 categories. As with two categories, we will have a reference group (typically the first group alphanumerically unless we specify otherwise), and then slopes for each group which represent the difference in means between the reference group and that group. A regression with a categorical variable with more than two groups is similar to an ANOVA, although it gives slightly different information. Let us re-do the ANOVA we did before, examining the relation between Openness and which continent a person is from, but this time in the regression framework. model3 = lm(OpennessSum ~ continent, data = big5) summary(model3) ## ## Call: ## lm(formula = OpennessSum ~ continent, data = big5) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.40 -2.38 0.49 2.60 12.62 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 33.4000 0.3919 85.223 &lt; 2e-16 *** ## continentAmericas -0.1600 0.5543 -0.289 0.77295 ## continentAsia -1.4900 0.5543 -2.688 0.00742 ** ## continentEurope -0.0200 0.5543 -0.036 0.97123 ## continentOceania 0.1100 0.5543 0.198 0.84276 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.919 on 495 degrees of freedom ## Multiple R-squared: 0.02277, Adjusted R-squared: 0.01487 ## F-statistic: 2.883 on 4 and 495 DF, p-value: 0.02219 As before, R has re-coded our variables to set one group as the reference group (in this case, Africa), and then has created indicator variables for all other continents. (Intercept): The intercept represents the expected value (or mean) of our outcome variable for our reference group. Thus, 33.40 is the average value of Openness for participants from Africa. Slopes: Each slope represents the difference in average Openness between the reference group and the group represented by the slope. The significance test will tell us whether these differences are significant or not (i.e., allows us to conduct pairiwse comparisons). continentAmericas: The average level of Openness for participants from the Americas is 0.16 points lower than the average level of Openness for participants from Africa. continentAsia: The average level of Openness for participants from Asia is 1.49 points lower than the average level of Openness for participants from Africa. continentEurope: The average level of Openness for participants from Europe is 0.02 points lower than the average level of Openness for participants from Africa. continentOceania: The average level of Openness for participants from Oceania is 0.11 points higher than the average level of Openness for participants from Africa. You will notice that the F-statistic for the model (2.88), df (4 and 495), and p-value (.02) is the same as the F-statistic, df, and p-value from the ANOVA we calculated earlier. So we can get the same conclusion as the ANOVA before (e.g., there is a difference in Openness across continents), but unlike the ANOVA, we don’t have to conduct a separate post-hoc test, as the pairwise comparison between the reference and other groups is already done. However, you’ll notice that the only comparisons we have are between the reference group and other groups. For example, we no information on whether the mean of Openness for participants from the Americas is different than the mean of Openness for European participants. Therefore, the choice between an ANOVA (and post-hoc tests) versus a regression might depend on which group comparisons you’d like to make in the end. Side note: You might notice that some of the results of the pairwise comparisons are different from what we calculated before (e.g., the difference between Africa and Asia is now significant, even though it wasn’t before). This is because the Tukey test corrects for multiple comparisons, and uses a slightly different reference distribution than the regression model. 7.2 Multiple Regression When we have two or more predictor variables, we move from simple linear regression to multiple linear regression. This allows us to examine the effects each predictor variable has on the outcome, controlling for the other predictor variable(s). In other words, what is the relation between our predictor and outcome, after taking into account people’s scores on the other predictor variables. To estimate a multiple regression model, we do the exact same thing we did before with simple regression models. But now, we separate our different predictor variables using +. So if we wanted to estimate the effects both age and Agreeableness have on Neuroticism, we could do this as follows: multiplereg_model = lm(OpennessSum ~ age + AgreeablenessSum, data = big5) summary(multiplereg_model) ## ## Call: ## lm(formula = OpennessSum ~ age + AgreeablenessSum, data = big5) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.8640 -2.6121 0.0639 2.5997 13.3627 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.085295 1.617036 17.987 &lt;2e-16 *** ## age 0.009449 0.014931 0.633 0.5271 ## AgreeablenessSum 0.116943 0.048255 2.423 0.0157 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.932 on 497 degrees of freedom ## Multiple R-squared: 0.01233, Adjusted R-squared: 0.008352 ## F-statistic: 3.101 on 2 and 497 DF, p-value: 0.04586 How to interpret these parameters? (Intercept): The intercept has the same interpretation as before - it is the expected value of our outcome variable when all predictors in our model are equal to 0. So in this case, the intercept of 29.09 is the expected Neuroticism score for someone who is 0 years old (age = 0) and has an Agreeableness score of 0. The slopes in a multiple regression model are generally interpreted the same way as in a simple regression model. The only difference is that now the slope represents the change in the outcome for a 1-unit change in our predictor, holding all other predictors constant. The reason we want to hold other predictors constant is so that we can isolate the effect of the single predictor variable, so the only thing that should be changing is the one predictor we’re interpreting. age: The slope of 0.01 means that for every 1-year increase in a person’s age, their Neuroticism score is expected to increase by 0.01 points, holding their Agreeableness score constant. One way you could think about this is, if we had two people with the exact same Agreeableness score (say, for example, they both had a score of 20) but one person was 25 and one person was 26, we would expect the 26-year-old to have a Neuroticism score that is 0.01 points higher than the score of the 25-year-old. The slope of age is not significant, so there is no relation between Neuroticism and age after taking into account people’s level of Agreeableness. AgreeablenessSum: The slope of 0.12 means that for every 1-point increase on a person’s Agreeableness score, their Neuroticsm score is expected to increase by 0.12 points, holding age constant. This slope is significant, so there is a relation between Agreeableness and Neuroticism even after accounting for age. Adjusted R-Squared: As before, we also have a \\(R^2\\) value that tells us the proportion of variance in our outcome that is explained by our predictors. However, we want to report the Adjusted R-Squared, as this accounts for having multiple predictors in the model (which will increase \\(R^2\\), even if the predictors are just explaining noise). So less than 1% of the variation in Neuroticism is explained by age and Agreeableness. 7.2.1 Higher-Order Polynomials Another type of multiple regression is including polynomial terms in your model (e.g., quadratic or cubic terms). In this case, although you do have multiple predictors (e.g., a linear term and quadratic term), the predictor itself is actually the same, just raised to different powers. Polynomial models are the simplest way to account for potential nonlinear relations between your predictor and outcome variable, but the interpretations of the model parameters become a little bit more complicated. To include a polynomial term in the model, you just write your predictor raised to whatever power you want to include (e.g., x^2), but you have to include it as the argument to a function called I(), so that R doesn’t think the ^ is part of the formula, but instead tells R to treat ^ “as is” (basically, raise the predictor to that power like we intended). Let’s see this in our example of the relation between age and Neuroticism, only know we think this relation is better described with a quadratic model. quadmod = lm(NeuroSum ~ age + I(age^2), data = big5) summary(quadmod) ## ## Call: ## lm(formula = NeuroSum ~ age + I(age^2), data = big5) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.4294 -5.1183 -0.2755 4.9175 16.9716 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.996261 1.927492 19.713 &lt; 2e-16 *** ## age -0.363584 0.124407 -2.923 0.00363 ** ## I(age^2) 0.003395 0.001734 1.958 0.05082 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.815 on 497 degrees of freedom ## Multiple R-squared: 0.05209, Adjusted R-squared: 0.04827 ## F-statistic: 13.65 on 2 and 497 DF, p-value: 1.687e-06 Not so hard! How do we interpret each of these parameters? Intercept: The intercept interpretation remains the same before: it is the expected value on our outcome when all predictors are 0. Therefore, 38.00 is the expected Neuroticism score for someone who is 0 years old (as this leads to both predictors to be 0). If we wanted this to be more interpretable, we could mean-center age like we did before. Linear slope of age (age): The linear slope of age (-0.36) has changed interpretation. It is no longer the change in Neuroticism for a 1-year change in age, but it is the slope of the tangent line (or the rate of change) at our intercept (when our predictors are 0). In other words, for someone who is 0 years old, we would expect the relation between age and Neuroticism to have a slope of -0.36. This can be seen as the slope of the dashed line in the plot below - the dashed line represents the tangent line at age = 0. Quadratic slope of age (I(age^2)): The quadratic slope of age can be viewed as the interaction between age and itself - in other words, how does the relation between age and Neuroticism change at different values of age? ggplot(data = big5, aes(x = age, y = NeuroSum))+ geom_point(alpha = .1)+ geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 2), se = FALSE)+ geom_abline(intercept = 37.996, slope = -0.364, linetype = &quot;dashed&quot;)+ theme_classic()+ labs(x = &quot;Age&quot;, y = &quot;Neuroticism&quot;) 7.2.2 Interactions The final aspect of regression models we will cover is interactions. Interactions are useful for studying moderation effects, when we think that the relation between a predictor variable and outcome variable depends on the level of a second predictor variable. This is typically done (or easiest to interpret) with one continuous and one categorical variable (e.g., the relation between the continuous predictor and continuous outcome is different depending on which group you are in) but can be done when both predictors are continuous or both categorical (for both variables being categorical, refer back to the section on interactions in anova). To specify interactions in regression models, you can simply use an asterisk (*) between the predictors you want to specify an interaction between. You do not have to specify the predictors separately from the interaction: writing out predictor1*predictor2 is interpreted by R as wanting both main effects (the effect of predictor1, and the effect of predictor2) as well as their interaction. Let us see this in an example, looking at how Neuroticism predicts Conscientiousness, and how this relation is moderated by participant’s gender. big5_subset2 = big5 %&gt;% filter(gender == 1 | gender == 2) # categorical variables should be of class &quot;factor&quot; or &quot;character&quot; to run appropriately; right now, gender is of class numeric big5_subset2$gender = as.factor(big5_subset2$gender) # specify the interaction between Neuroticism and gender with NeuroSum*gender multiplereg_interaction = lm(ConscSum ~ NeuroSum*gender, data = big5_subset2) summary(multiplereg_interaction) ## ## Call: ## lm(formula = ConscSum ~ NeuroSum * gender, data = big5_subset2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.966 -2.292 -0.158 2.281 11.737 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.54975 1.09220 27.971 &lt;2e-16 *** ## NeuroSum 0.02854 0.03507 0.814 0.4162 ## gender2 -3.50497 1.47966 -2.369 0.0182 * ## NeuroSum:gender2 0.11208 0.04661 2.405 0.0166 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.58 on 492 degrees of freedom ## Multiple R-squared: 0.04231, Adjusted R-squared: 0.03647 ## F-statistic: 7.245 on 3 and 492 DF, p-value: 9.166e-05 You can see in our output that we have estimates for the intercept ((Intercept)), the two main effects of our predictor variables (NeuroSum and gender) and the interaction between the two (NeuroSum:gender). Let’s interpret each of these terms: (Intercept): As before, the intercept is the expected value of our outcome when all predictors in our model are equal to 0. Notice that if both Neuroticism and gender are equal to 0, then the interaction term (which is created by multiplying Neuroticism by gender) is also equal to 0. So a male (gender = 0) who has a score of 0 on Neuroticism is expected to have a Conscientiousness score of 30.55. Main effects of Neuroticism and gender: In a multiple regression model, when we interpreted one slope, we just had to state that the other predictor variable was being held constant, but it did not matter which value it was held constant at. Now, with the interaction term in our model, we have to specify that the other predictor variable is held constant at 0 - this is because a change in our predictor variable of interest will affect both the slope we’re interested in and and the interaction term, which does not allow us to isolate the main effect unless the interaction term is set to 0. Also, many researchers say that if the interaction term in a model is significant, the main effects should not be interpreted. This is because a significant interaction term means that the effect of one predictor on the outcome (a main effect) depends on the level of the other predictor, so what is the point in interpreting the main effect without mentioning the interaction. If we were to interpret it anyways, the main effect of Neuroticism (-0.08) means that when gender = 0 (the participant is male), a 1-point increase in Neuroticism will lead to a 0.08 decrease in Conscientiousness score. The main effect of gender means that, when the Neuroticism score is 0, females are expected to have average Conscientiousness scores that are 3.50 points lower than males. The interaction term is significant. The value of the interaction term is the difference in slopes representing the effect of Neuroticism on Conscientiousness between males and females. Since it is significant, the effect of Neuroticism on Conscietiousness is different between males and females. Now that we know the relation between Neuroticism and Conscientiousness depends on gender, we might want to know how the relation is different for males and females. You can do this visually (as we will do below), but also by conducting a “simple effects” analysis, where you re-run the regression model separately for each group. # re-run the regression between Neuroticism and Conscientiousness for males model_male = lm(ConscSum ~ NeuroSum, data = big5 %&gt;% filter(gender == 1)) # re-run the regression between Neuroticism and Conscientiousness for females model_female = lm(ConscSum ~ NeuroSum, data = big5 %&gt;% filter(gender == 2)) summary(model_male) ## ## Call: ## lm(formula = ConscSum ~ NeuroSum, data = big5 %&gt;% filter(gender == ## 1)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.4345 -2.2918 -0.1919 1.9009 8.3943 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.54975 1.09648 27.862 &lt;2e-16 *** ## NeuroSum 0.02854 0.03521 0.811 0.419 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.594 on 196 degrees of freedom ## Multiple R-squared: 0.003341, Adjusted R-squared: -0.001744 ## F-statistic: 0.6571 on 1 and 196 DF, p-value: 0.4186 summary(model_female) ## ## Call: ## lm(formula = ConscSum ~ NeuroSum, data = big5 %&gt;% filter(gender == ## 2)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.9665 -2.2595 -0.1384 2.4632 11.7366 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.04479 0.99565 27.163 &lt; 2e-16 *** ## NeuroSum 0.14062 0.03062 4.592 6.51e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.571 on 296 degrees of freedom ## Multiple R-squared: 0.06649, Adjusted R-squared: 0.06334 ## F-statistic: 21.08 on 1 and 296 DF, p-value: 6.508e-06 By examining the summary output, we can see that the relation between Neuroticism and Conscientiousness only exists (is significant) for females, whereas it is nonsignificant for males. 7.3 Visualizing Regression Lines When you visualize a regression model, it’s pretty standard to plot your observed data as a scatterplot, and then put your estimated regression line on top of that. For a simple regression with a continuous predictor, this is pretty straightforward (and if you have a categorical predictor, you can use the visualization methods you learned about for t-tests and ANOVAs). ggplot(data = big5, aes(x = age, y = NeuroSum))+ geom_point()+ # to plot a scatterplot of the data geom_smooth(method = &quot;lm&quot;, se = TRUE)+ # to overlay the estimated regrssion line along with the standard error of the line theme_classic()+ labs(x = &quot;Age&quot;, y = &quot;Neuroticism&quot;, title = &quot;Age versus Neuroticism&quot;) Things get a little more complicated once you have have multiple predictor variables. We will talk about the situation with 2 predictor variables (one continuous predictor, and the other predictor is either categorical, continuous, or a polynomial term). These approaches can then be extended if you have more than 2 predictor variables, although it’s likely to get a lot messier. 7.3.1 Visualizing Multiple Regression: One Continuous Predictor, One Categorical When you have one continuous predictor and one categorical predictor, you can simply plot separate regression lines showing the relationship between the continuous predictor and outcome at each level of the categorical variable. These regression lines can be placed within the same plot, or in separate plots. Let’s suppose we were trying to plot the interaction model we ran earlier, where we predicted Conscietiousness based on Neuroticism and gender (where for gender we only considered males and females, due to the low number of participants who reported their gender as non-binary). Therefore, we will use the big5_subset2 dataset for our plot, since that is the dataset where (1) gender has been subsetted to only males and females and (2) gender has been transformed to a factor. In these plots, the continuous predictor is typically placed on the x-axis. Then, if all regression models are placed in the same plot, different colors are used to represent the different levels of the categorical variables. If the regression models are in separate plots, we use a function called facet_wrap() to split up the plots based on a categorical variable. # regression lines at each level, in the same plot ggplot(data = big5_subset2, aes(x = NeuroSum, y = ConscSum, color = gender))+ # we add color = gender so that our datapoints and regression # lines have a different color for each group geom_point()+ geom_smooth(method = &quot;lm&quot;, se = TRUE)+ theme_classic()+ # to make the labels for color more informative, we can use # scale_color_discrete(), which lets us change the values of # a discrete variable that we are using different colors for scale_color_discrete(labels = c(&quot;Male&quot;, &quot;Female&quot;))+ labs(x = &quot;Neuroticism&quot;, y = &quot;Conscientiousness&quot;, color = &quot;Gender&quot;) # regression lines at each level, across different plots # in this case, you might want to change the values of the variable you are splitting into different plots so that the label is informative # you can do this by directly modifying the dataset, but I will show how to do this by using an argument called labeller # what do you want the labels to be? gender.labs = c(&quot;Male&quot;, &quot;Female&quot;) # what are the labels currently? Make sure the order matches with the order you used for the labels above? names(gender.labs) = c(&quot;1&quot;, &quot;2&quot;) ggplot(data = big5_subset2, aes(x = NeuroSum, y = ConscSum))+ # because we will be splitting up the regression lines for each gender into different plots, we don&#39;t necessarily need to add different colors for each gender, but we could! facet_wrap(~ gender, labeller = labeller(gender = gender.labs))+ # in facet wrap, you can split your graph into different plots # based on a categorical variable # variables before the ~ will be arranged vertically, and variables # after the ~ will be arranged horizontally # then to add labels, you will use the argument: # labeller = labeller(faceting.variable = variable.labels) geom_point()+ geom_smooth(method = &quot;lm&quot;, se = TRUE)+ theme_classic()+ labs(x = &quot;Neuroticism&quot;, y = &quot;Conscietiousness&quot;) The choice between plotting both regression lines into the same plot versus different plots depends on the goals of your research - it is easier to compare regression lines if they are on the same plot, but that can get a little bit messy (especially when you have more than 2 categories). However, we as we can see in either regression plot, the relationship between Neuroticism and Conscientiousness is stronger for females than males (the line is steeper), which is what we got from our simple effects analysis. 7.3.2 Visualizing Multiple Regression: Two Continuous Predictors When you have two continuous predictor variables, the visualization is not as clean-cut, because there are no built-in categories that you can split your plots by. However, you might still be interested in creating this visualization, particularly when you have an interaction between two continuous variables. Therefore, it is pretty standard that when you plot the interaction between two continuous variables, that you form a categorical variable out of one of your continuous variables for illustrative purposes of how the relation between the remaining continuous predictor and outcome changes. (I am bolding this because normally we don’t want to categorize continuous variables!). The categorical variable you form is typically for people who are at the average level of the continuous variable, 1 SD above the average, and 1 SD below the average. Let us suppose we wanted to look at how Openness (our outcome variable) is predicted by Agreeableness and age. To visualize this, we will get the predicted values for the relation between Openness and Agreeableness at different ages (the average age, 1 SD above the average age, and 1 SD below the average age). In order to use the predict function, we need to provide it with two things: a lm model, and a dataset with the input values to generate predictions for. In this case, we want to use the original Agreeableness values from our dataset, but want to set age to one of three specific values. Therefore, we will create three new datasets, one for each value of age, and then predict from that. # create a dataset with Agreeableness and average age big5_averageage = big5[ , c(&quot;id&quot;, &quot;AgreeablenessSum&quot;)] # now we create a column that contains the average value of age # it has to be called age to match the name of the predictor in our linear model big5_averageage$age = mean(big5$age, na.rm = TRUE) # get the predicted values if we used the values in this dataset as the input # predict(modelname, newdataset) multiplereg_interaction = lm(OpennessSum ~ AgreeablenessSum*age, data = big5) big5_averageage_predict = predict(multiplereg_interaction, big5_averageage) # these predicted values show the relation between Agreeableness and Openness in our dataset, for someone who is average age # Repeat the process above for someone who is 1 SD below the average age big5_sdbelow = big5[ , c(&quot;id&quot;, &quot;AgreeablenessSum&quot;)] big5_sdbelow$age = mean(big5$age, na.rm = TRUE) - sd(big5$age, na.rm = TRUE) big5_sdbelow_predict = predict(multiplereg_interaction, big5_sdbelow) # Repeat the process above for someone who is 1 SD above the average age big5_sdabove = big5[ , c(&quot;id&quot;, &quot;AgreeablenessSum&quot;)] big5_sdabove$age = mean(big5$age, na.rm = TRUE) + sd(big5$age, na.rm = TRUE) big5_sdabove_predict = predict(multiplereg_interaction, big5_sdabove) # Now, to graph this, we gather all these predictions into one dataframe, along with the original Openness, Agreeableness, and age values big5_plot = big5[, c(&quot;id&quot;, &quot;OpennessSum&quot;, &quot;AgreeablenessSum&quot;, &quot;age&quot;)] big5_plot$meanage_predict = big5_averageage_predict big5_plot$sdbelow_predict = big5_sdbelow_predict big5_plot$sdabove_predict = big5_sdabove_predict # However, we need to get all the predicted values into one column, and then have one column to tell us which age value these predictions are at # This way, we can color our prediction lines # to do this, we have to &quot;pivot&quot; our data into long format (collapsing values from multiple columns down into 1) # important arguments: # data = the dataset you want to pivot # columns = the columns you want to collapse down into one # names_to = the name of the column that contains the old column names # values_to = the name of the column you want the column values to go into big5_plot = tidyr::pivot_longer(data = big5_plot, cols = c(&quot;meanage_predict&quot;, &quot;sdbelow_predict&quot;, &quot;sdabove_predict&quot;), names_to = &quot;agegroup&quot;, values_to = &quot;predicted&quot;) head(big5_plot) # Now we can make a plot, with different colored lines for each age ggplot(data = big5_plot, aes(x = AgreeablenessSum, y = OpennessSum))+ geom_point()+ geom_line(aes(x = AgreeablenessSum, y = predicted, color = agegroup))+ theme_classic() # Now we can see that we have three different lines - one showing the relation between Agreeableness and Openness for someone who is average age, one showing the relation for someone who is 1 SD above average age, and one for someone who is 1 SD below average age # Let&#39;s make the graph a bit nicer by changing the labels on the axes, as well as the labels for the different age groups ggplot(data = big5_plot, aes(x = AgreeablenessSum, y = OpennessSum))+ geom_point()+ geom_line(aes(x = AgreeablenessSum, y = predicted, color = agegroup))+ scale_color_discrete(labels = c(&quot;Average Age&quot;, &quot;1 SD Above&quot;, &quot;1 SD Below&quot;))+ # the order you put the new labels in has to match the current order (which is alphabetical, unless you changed the factor levels to be otherwise) theme_classic()+ labs(x = &quot;Agreeableness&quot;, y = &quot;Openness&quot;, color = &quot;Age&quot;) 7.3.3 Visualizing Multiple Regression: Polynomials Polynomials are a special case of multiple regression, in the sense that we’re not looking at how the relation between Predictor 1 and the outcome changes at the different levels of Predictor 2, we’re just trying to plot one regression line like we did with simple regression. Luckily, this just requires a small change to the geom_smooth part of the ggplot2 code! Suppose we wanted to plot the polynomial model we had estimated before: where \\(age\\) and \\(age^2\\) were both predictors of Neuroticism. ggplot(data = big5, aes(x = age, y = NeuroSum))+ geom_point()+ # plot the data geom_smooth(method = &quot;lm&quot;, se = TRUE, formula = y ~ poly(x, 2))+ # even though our predictor was age and outcome was Neuroticism, use x and y in the formula argument theme_classic()+ labs(x = &quot;Age&quot;, y = &quot;Neuroticism&quot;, title = &quot;Quadratic Relation between Age and Neuroticism&quot;) 7.4 Checking Assumptions of the Regression Model Linear regression makes 4 main assumptions: Observations are independent from each other The relation between the predictor and outcome is linear The residuals of the model are normally distributed The residuals of the model have equal/constant variance (this is called homogeneity of variance or homoskedasticity) The first assumption is normally determined based on how you collected your data - is it reasonable to assume that different observations are independent from each other? In other words, the response of one observation has no impact on the response on another observation. To check assumptions 2-4, we can use a visual check, as well as formal statistical tests for assumptions 3 and 4. To get the visual plots to check these assumptions, we use the built-in plot function. plot(model1) # the input of the plot() function is the model we want to check assumptions for This returns four plots, but really we just need to look at the first 2 to examine our assumptions. The first plot (residuals versus fitted) can tell us whether assumption 2 (linear relation) and assumption 4 (homogeneity of variance) is met, and the second plot (normal QQ-plot) can tell us about assumption 3 (normality of residuals). To check for linearity assumption: is the red line in the residuals versus fitted plot roughtly flat at y = 0 (does it match the gray dashed line)? If so, this assumption is met. In our example, the red line appears pretty flat, with maybe a slight uptick at the end, but on the whole I would say this supports the assumption that the relation is linear. To check for homogeneity of variance: Does the spread of the residuals in the residuals vs fitted plot look pretty even, and the points are randomly scattered around the line y = 0? In other words, there is no clear pattern in the residuals versus fitted plot, or a fan-shape? If there is a pattern or fan-shape, this indicates the model is doing a better job predicting at certain values of your predictor than others, which we don’t want. In our example, the spread of the residuals looks pretty even and random across the fitted values, so homogeneity of variance is met. To check or normality of residuals: Do the points in the normal QQ-plot fall along the dashed line y = x? If so, then the normality of residuals assumption is met. Again, although there is a slight deviation from the line at the top right of the graph (indicating the residuals might be slightly positively skewed), the plotted points roughly match the y = x line, so the residuals can be assumed to be normally distributed. However, these checks of the assumptions can be more based on your subjective opinion. Two people could look at the same graphs and potentially come to different conclusions about whether or not the assumptions are met. So do more formal ways of checking these assumptions exist? You can conduct White’s Test for Heterskedasticity to check for homogeneity of variance, and a Shapiro-Wilks test to check for normality of residuals. For both these tests, the null hypothesis is that the assumption is met (e.g., there is homogeneity or there is normality), so you want to fail to reject the null hypothesis in order to meet the assumptions. To conduct White’s Test, you can use the white function from the package skedastic, and for Shapiro-Wilks Test, you can use the shapiro.test function that is built into R. white(model1) # for White&#39;s test, give the function your lm model as an argument shapiro.test(model1$residuals) # for the Shapiro-Wilks test, you need to give it the model residuals specifically ## ## Shapiro-Wilk normality test ## ## data: model1$residuals ## W = 0.99231, p-value = 0.01117 So the p-value for White’s test was \\(p = .67\\), which means we fail to reject the null hypothesis, and we assume we have met the assumption of homogeneity of variance. The p-value for Shapiro-Wilk test was \\(p = .01\\), so we reject the null hypothesis. Therefore, although our visual inspection led us to believe we had normally-distributed residuals, based on this test we actually have non-normally distributed residuals. If the assumptions are violated, you can apply data transformations like you learned about in 4 to try and make your residuals more normally distributed. "],["multilevel.html", "8 Multilevel Modeling 8.1 Clarifying Some Terminology 8.2 The Intra-Class Correlation Coefficient 8.3 Running a Multilevel Model 8.4 Visualizing a Multilevel Model", " 8 Multilevel Modeling Remember that one of the assumptions of linear regression that we learned about in the last chapter was that our observations are independent from each other. However, in your research, you may be working with data that are instead clustered in some way - for example, you might have responses from students within the same classroom (students are clustered within the classroom), or repeated measures from the same individual across time or different trials in an experiment (multiple observations are clustered within the same individual). In these cases, your data are no longer independent - observations from the same cluster are more likely to be similar to each other than observations from different clusters. This would be most obvious for repeated measurements from the same individual across time/trials - a single person is likely to respond or behave similarly across timepoints/trials, compared to the responses from a completely different person. When you have clustered data like this, but you want to run a regression model, you would instead want to use a multilevel model (this can also be called hierarchical linear modeling or mixed effects models). This is because if you run a normal linear regression when you have clustered data, your model ignores the fact that your data are no longer independent, and so the standard errors it estimates are too small (which could lead to your inferential tests being too liberal!). There are two popular packages you can use to run multilevel models in R: lme4 and nlme. I will focus on using lme4, although similar results can be obtained from nlme (which is more suited to nonlinear models). If you want more details on lme4, you can read this tutorial. library(ggplot2) # for general data visualization library(ggpubr) # for arranging plots library(performance) # to calculate ICC library(lme4) # for multilevel modeling library(lmerTest) # useful for inferential tests in multilevel models, since lme4 does not return p-values 8.1 Clarifying Some Terminology As we discuss multilevel models, there are some two terms that need to be defined first: Fixed effects: Fixed effects refer to the coefficients representing the relation between the predictors and outcome variable, averaged across clusters or groups. In other words, fixed effects describe the average values of the estimated parameters. Random effects: Random effects describe which parameters are allowed to vary across the different groups or clusters. Not all parameters have to vary across groups or clusters - you can specify based on what you think is most likely. Now those terms have been defined, let’s work through some multilevel models. For our examples, suppose that we wanted to examine the relation between age and Neuroticism (like we did before), but this time, we want to account for the fact that participants are clustered within continents (people from the same continent are likely to be more similar than people from other continents). 8.2 The Intra-Class Correlation Coefficient Oftentimes, before running a multilevel model, it is useful to calculate an intra-class correlation coefficient (ICC). The intra-class correlation coefficient tells you how much of the variability in your outcome is due to the variability across different clusters. It ranges from 0 (no dependency in the data due to continents, e.g., people from the same continent are equally as similar as people from different continents) to 1 (complete dependency due to continents, so people from the same continent are exactly alike and are different than people from other continents), The rule of thumb for the ICC is that greater than .05 is considered substantial variation (and thus, is an indication that a multilevel model might be more appropriate). To calculate the ICC, we first need to run a model that contains only an intercept for our outcome, but also has a random intercept. This tells us how much difference there is in the expected value of our outcome across groups. We can then use the icc function from the performance package. When specifying a mutlievel model with the function lmer, you write the regression formula like you normally would (e.g., dependent ~ indepdendent). To add random effects (i.e., to let parameters vary across groups/clusters), you simply add them in parentheses, by writing + (whatever parameters you want random effects on, separated by + signs | clustering variable). # 1 represents the intercept, so I have no predictors in my model, just an intercept, but I want that intercept to vary across continents interceptonly = lmer(NeuroSum ~ 1 + (1 | continent), data = big5) icc(interceptonly) The ICC is 0.016, which means that about 1.6% of the variance in our outcome is due to differences between clusters (in this case, continents). This is less than the rule of thumb value of .05 that I mentioned before, but we will continue with our example regardless. 8.3 Running a Multilevel Model 8.3.1 Simple Multilevel Model First, to provide a point of comparison, let’s run the regression model without accounting for the clustering. Note that I am using centered age in the model, just to have better interpretation of the intercept! nocluster_model = lm(NeuroSum ~ age_centered, data = big5) summary(nocluster_model) ## ## Call: ## lm(formula = NeuroSum ~ age_centered, data = big5) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.1376 -5.1411 -0.0165 4.8568 16.6089 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 31.17400 0.30565 101.994 &lt; 2e-16 *** ## age_centered -0.12535 0.02594 -4.832 1.81e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.834 on 498 degrees of freedom ## Multiple R-squared: 0.04478, Adjusted R-squared: 0.04286 ## F-statistic: 23.34 on 1 and 498 DF, p-value: 1.806e-06 Now, let’s run the multilevel model. We will include random effects for both the intercept and the slope. What this means is that we think that different continents have different baseline levels of Neuroticism (random intercept), and that the relation between age and Neuroticism can differ across continents. As a reminder, the way that we can specify random effects in lmer is by first specifying our usual regression model (e.g., NeuroSum ~ age), but then in parentheses, adding (1 + age | continent). The parentheses indicates that these are random effects, the | continent says that the grouping or clustering variable is the continent, and 1 indicates a random intercept while age indicates a random age effect. cluster_model = lmer(NeuroSum ~ age_centered + (1 + age_centered | continent), data = big5) summary(cluster_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: NeuroSum ~ age_centered + (1 + age_centered | continent) ## Data: big5 ## ## REML criterion at convergence: 3342.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.90826 -0.76895 -0.01166 0.71718 2.36771 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## continent (Intercept) 0.448075 0.66938 ## age_centered 0.001588 0.03986 -1.00 ## Residual 46.193981 6.79662 ## Number of obs: 500, groups: continent, 5 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 31.13101 0.42713 4.54254 72.885 3.81e-08 *** ## age_centered -0.12264 0.03155 4.74889 -3.887 0.0128 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## age_centerd -0.397 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) What does this output tell us? The fixed effects tell us the average effects in our population - e.g., on average, across continents, what is the intercept and slope of our model? These can be interpreted like normal regression coefficients: the intercept represents the expected level of Neuroticism for someone who is of average age, while the slope of age tells us that for every 1 year change in age, we expect a person’s Neuroticism to decrease by 0.12 points. The random effects aren’t actually parameters that get estimated: instead, they are variances (and covariances) of the parameters that we have allowed to differ across continents. That is, the variance of the intercept term tells us how much we expect the intercept to vary across continents, the variance of the slope term tells us how much we expect the slope of age to differ across continents, and the correlation tells us how we expect a country’s intercept to relate to their slope of age. Note: You might see that we had a warning message in our model - that the fit of our model is “singular”. This is likely because the variance of our random slope is close to 0 (.002), indicating that this slope doesn’t really differ across continents. This can lead to the singularity warning. How does this differ from the regression results when we did not account for the clustering? ## ## OLS versus Multilevel Regression Results ## ============================================================ ## Dependent variable: ## ---------------------------------------- ## Neuroticism ## OLS linear ## mixed-effects ## OLS Model Multilevel Model ## ------------------------------------------------------------ ## Constant 31.174*** 31.131*** ## (0.306) (0.427) ## ## age_centered -0.125*** -0.123*** ## (0.026) (0.032) ## ## ------------------------------------------------------------ ## Observations 500 500 ## R2 0.045 ## Adjusted R2 0.043 ## Log Likelihood -1,671.466 ## Akaike Inf. Crit. 3,354.932 ## Bayesian Inf. Crit. 3,380.220 ## Residual Std. Error 6.834 (df = 498) ## F Statistic 23.344*** (df = 1; 498) ## ============================================================ ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 You’ll notice that our intercept and slope parameters are actually pretty much the same - in both cases, the intercept is around 31, and the slope is around -0.12. The only differences are in the standard errors of these parameters - the standard errors of the intercept and slope in the multilevel model (0.43 and 0.032, respectively) are larger than the standard errors in the regression model (0.31 and 0.026, respectively). In this case, this difference did not change the inferential results (e.g., the effect of age is still significant), but you could imagine that having a standard error that is too small may lead us to reject the null hypothesis incorrectly. To visually illustrate the differences between a regression model, or a model with random intercept and slope, look at the graphs below. In these graphs, the thick blue line represents the average trajectory - e.g., across all continents, what is the predicted relation between age and neuroticism. However, in the OLS model on the far right, the single line shown means that we expect this relation to hold for any individual in our sample, regardless of what continent they are from. In the model with a random intercept (middle), different continents are allowed to have different intercepts (random intercept) but the relation between age and neuroticism remains the same (fixed slope). This can be seen in how the different colored lines for each continent are parallel to each other, but just shifted vertically. Finally, the model with random intercept and random slope (far left), different continents are allowed to have both different intercepts and different slopes, which is represented by how the different colored lines have different starting points and different levels of steepness. 8.4 Visualizing a Multilevel Model Since a multilevel model is just a regression model that accounts for clustering, you can use all the techniques we learned about in the previous chapter to visualize the regression models (e.g, using geom_smooth). However, we might also want to include different lines for each cluster (in this case, continent) to see how the intercepts and slopes differ across our different clusters. To do this, we just add a new column to our dataset that includes the predicted values for each individual. These predicted values will take into account the differences in intercept/slope across the different continents, so we can just use different colored lines for each continent to see the differences in predicted regression equations. # add a column of predicted values to your dataset big5$mlm_predict = predict(cluster_model) # the outcome variable is now our predicted values ggplot(data = big5, aes(x = age, y = mlm_predict))+ geom_line(aes(color = continent), alpha = 0.5)+ # individual lines for each continent geom_smooth(method = &quot;lm&quot;, linewidth = 0.75, se = FALSE, color = &quot;black&quot;)+ # a predicted regression equation - this will be the regression equation based on fixed effects theme_classic()+ labs(x = &quot;Age&quot;, y = &quot;Predicted Neuroticism&quot;, color = &quot;Continent&quot;, title = &quot;Relation between Age and Neuroticism by Continent&quot;) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
